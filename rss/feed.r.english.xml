<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>En El Margen - R-English</title>
		<description>Posts categorized as 'R-English'</description>
		<link>http://localhost:4000</link>
		<atom:link href="http://localhost:4000/rss/r.english.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>conapomx data package</title>
					<description>&lt;p&gt;I have created a new data package for R to help users obtain national population statistics of Mexico.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;conapomx&lt;/strong&gt; package contains the &lt;em&gt;mxpopulation&lt;/em&gt; data set, which is a tidy data.frame containing population estimates from the CONAPO (National Population Commission) official agency. The estimates are divided by age groups, gender, municipality and year.&lt;/p&gt;

&lt;p&gt;To install, just call CRAN (or github if you are reading this before it is accepted).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;install.packages(&amp;quot;conapomx&amp;quot;) 
# or... 
library(devtools)
install_github(&amp;quot;eflores89/conapomx&amp;quot;)

# explore the dataset
head(mxpopulation)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;y&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;st&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;id_st&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;mun&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;id_mun&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;geoid&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;gender&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;age&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;population&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2010&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1001&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0-14&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;124263.71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2010&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1001&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;15-29&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;106695.14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2010&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1001&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;30-44&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;81088.65&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2010&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1001&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;45-64&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60379.29&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2010&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1001&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;65+&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;17679.26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2010&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1001&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0-14&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;119535.77&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here is the data for 2018, just out of curiosity:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(dplyr)

mxpopulation %&amp;gt;% 
  filter(y == &amp;quot;2018&amp;quot;) %&amp;gt;%
  group_by(st) %&amp;gt;% 
  summarize(&amp;quot;Population&amp;quot; = sum(population))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;st&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Population&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Aguascalientes&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1337792.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Baja California&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3633772.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Baja California Sur&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;832827.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Campeche&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;948459.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Chiapas&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5445232.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Chihuahua&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3816865.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Coahuila&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3063662.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Colima&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;759686.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Distrito Federal&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8788140.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Durango&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1815965.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Guanajuato&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5952086.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Guerrero&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3625040.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hidalgo&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2980532.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Jalisco&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8197483.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Mexico&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;17604619.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Michoacan&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4687210.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Morelos&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1987595.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Nayarit&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1290518.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Nuevo Leon&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5300618.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Oaxaca&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4084674.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Puebla&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6371380.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Queretaro&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2091823.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Quintana Roo&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1709478.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;San Luis Potosi&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2824976.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Sinaloa&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3059321.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Sonora&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3050472.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tabasco&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2454294.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tamaulipas&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3661161.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tlaxcala&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1330142.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Veracruz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8220321.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Yucatan&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2199617.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Zacatecas&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1612014.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Hopefully this helps to avoid the notoriously bad &lt;em&gt;datos.gob&lt;/em&gt; webpage! Happy data wrangling!&lt;/p&gt;
</description>
				<pubDate>Tue, 16 Jan 2018 06:00:30 +0100</pubDate>
				<link>http://localhost:4000/datascience/conapo-package/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/conapo-package/</guid>
			</item>
		
			<item>
				<title>NAFTA trade dashboard</title>
					<description>&lt;p&gt;&lt;strong&gt;Tradewatch&lt;/strong&gt; is a dashboard, written in &lt;strong&gt;R&lt;/strong&gt; using the &lt;a href=&quot;http://rmarkdown.rstudio.com/flexdashboard/&quot;&gt;flexdashboard&lt;/a&gt; framework. Visualizations are made possible thanks to  &lt;a href=&quot;http://jkunst.com/highcharter/index.html&quot;&gt;highcharter&lt;/a&gt; and other packages.&lt;/p&gt;

&lt;p&gt;The main purpose of the dashboard is to monitor trade, mostly from the Mexican perspective, of goods and commodities with countries that are in NAFTA.&lt;/p&gt;

&lt;p&gt;In the age of fake news and policy-by-twitter, hopefully this tool will help guide more informed debate (and decisions) with respect to trade between these three countries.&lt;/p&gt;

&lt;h2 id=&quot;notes-and-methodology&quot;&gt;Notes and methodology&lt;/h2&gt;

&lt;p&gt;The dashboard consists of basically three sections:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bilateral Trade&lt;/strong&gt;: These contain two, side-by-side comparisons, of exports and imports of products at a 2-level HS code depth. The color denotes year-over-year growth rate, while the size is related to the dollar value (in the newest month) of exports. The average growth rate for 2 digit codes is a weighted-mean of each 4-digit classification.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;US Share-shift:&lt;/strong&gt; A visualization that describes the shift in share in the U.S. market of Mexican goods relative to Canadian goods. When the shift is lower than 0, Mexican imports lost share (vs. Canada last year) in the United States, and vice-versa. &lt;em&gt;Important caveat: the share is calculated only for goods from Mexico or Canada (Because they are not collectively exhaustive, it is perfectly posible that &lt;strong&gt;both&lt;/strong&gt; lost share against a third competitor)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Terms of Trade &amp;amp; Fx:&lt;/strong&gt; Prices are important. This section aims to gauge the impact of prices in the trade dynamic. To obtain exchange rates, I use the &lt;a href=&quot;https://cran.r-project.org/web/packages/tidyquant/index.html&quot;&gt;tidyquant package&lt;/a&gt;. The other indexes refer to the price index of products imported from each of those countries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;The names of HS Codes have been abbreviated manually, they can be found &lt;a href=&quot;&quot;&gt;here&lt;/a&gt;, however the code is still in between parenthesis. Should you want to look it up further, I recommend the UN Comtrade website&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;suggestions&quot;&gt;Suggestions&lt;/h2&gt;

&lt;p&gt;This release is a first and &lt;em&gt;fast&lt;/em&gt; dive into the data, so I would be happy to listen to improvements to the tool, including some new indicators I haven’t added. &lt;em&gt;FYI, I tried to build the export matrixes at a 4-level granularity, but highcharts couldn’t handle it, any suggestions on how to do it would be welcome.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You can contact me via &lt;a href=&quot;https://twitter.com/eflores89&quot;&gt;twitter&lt;/a&gt; or at my blog’s &lt;a href=&quot;https://github.com/Eflores89/Eflores89.github.io/issues&quot;&gt;issue&lt;/a&gt; page.&lt;/p&gt;

&lt;h2 id=&quot;dashboards&quot;&gt;Dashboards&lt;/h2&gt;

&lt;p&gt;The newest I could build is from november 2016, which is the month all three countries have statistics ready.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://enelmargen.org/tradewatch/tw201611.html&quot;&gt;November 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;The code used to build the dashboard consists of two files: a &lt;strong&gt;functions.R&lt;/strong&gt; file that contains some functions for downloading and cleaning the data and a &lt;strong&gt;db.Rmd&lt;/strong&gt; file that builds the dashboard using the &lt;em&gt;knitr/flexdashboard&lt;/em&gt; process in RStudio.&lt;/p&gt;

&lt;p&gt;Both of these files can be found &lt;a href=&quot;http://enelmargen.org/tradewatch/functions.R&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://enelmargen.org/tradewatch/db.Rmd&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;FYI, I borrowed heavily from &lt;a href=&quot;https://comtrade.un.org/data/Doc/api/ex/r&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://gist.github.com/guillgall/3b979b59e8905ef7f244&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sun, 18 Jun 2017 21:00:30 +0200</pubDate>
				<link>http://localhost:4000/tradewatch/intro/</link>
				<guid isPermaLink="true">http://localhost:4000/tradewatch/intro/</guid>
			</item>
		
			<item>
				<title>Shiny App for MBA Interviews</title>
					<description>&lt;p&gt;A couple of weeks ago I attended some interviews as part of my process for admission into an MBA program. While most candidates would pore over the possible questions and then try to teach them to a third party to help them with a mock interview, I decided to expedite the process by building a shiny app in &lt;em&gt;R&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In the name of helping anyone out there who knows R and is applying to an MBA (I know it is probably very few people), here is the code and a simple explanation of my workflow.&lt;/p&gt;

&lt;h2 id=&quot;workflow&quot;&gt;Workflow&lt;/h2&gt;

&lt;p&gt;First, you will need an excel with all the possible questions to ask. I compiled one &lt;a href=&quot;http://enelmargen.org/docs/qs.xlsx&quot;&gt;here&lt;/a&gt;. You place this excel in the folder with the ui.R and server.R files. When a button is pressed on the app (the “score”), the shiny app will export a very small text file into that folder. This file will be separated by pipes and have 6 columns and one row: each indicating what question and school it is and the subjective score given by the interviewer to the question.&lt;/p&gt;

&lt;p&gt;When you want to analyze the results, you simply import each file and rbind them to have a nifty data.frame. I’ll also leave the code to do this later on.&lt;/p&gt;

&lt;p&gt;Now, on to the code.&lt;/p&gt;

&lt;h2 id=&quot;serverr&quot;&gt;server.R&lt;/h2&gt;

&lt;p&gt;Change it as you wish. There is a timer, that will indicate when time is running more the standard the interview suggests.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(shiny)
library(readxl)
library(shinyjs)
library(dplyr)
d &amp;lt;- read_excel(&amp;quot;qs.xlsx&amp;quot;)
jsResetCode &amp;lt;- &amp;quot;shinyjs.reset = function() {history.go(0)}&amp;quot;

shinyServer(function(input, output, session) {
  EventTime &amp;lt;- Sys.time()
  
  qqt &amp;lt;- reactive({
    if(input$response==&amp;quot;To interviewer&amp;quot;){
      d &amp;lt;- d %&amp;gt;% filter(RESPONSE==&amp;quot;To Interviewer&amp;quot;)
    }else{
      d &amp;lt;- d %&amp;gt;% filter(RESPONSE==&amp;quot;To applicant&amp;quot;)
    }
    
    qs &amp;lt;- d$Q
    qn &amp;lt;- sample(row.names(d),1)
    qr &amp;lt;- qs[as.numeric(qn)]
    if(grepl(pattern = &amp;quot;(\\[SCHOOL\\])&amp;quot;, x = qr)){
      qr &amp;lt;- gsub(pattern = &amp;quot;(\\[SCHOOL\\])&amp;quot;, replacement = input$school, x = qr)
    }
    as.character(paste0(&amp;quot;Q&amp;quot;,qn,&amp;quot;: &amp;quot;,qr))
  })
  
  output$qqt2 &amp;lt;- renderText({ 
    txt &amp;lt;- qqt()
    txt 
    })
 
  output$eventTimeRemaining &amp;lt;- renderUI({
    invalidateLater(1000, session)
    tm &amp;lt;- as.numeric(round(difftime(
      Sys.time(), EventTime, units = &amp;#39;secs&amp;#39;)))
    
    if(tm&amp;lt;input$NormalTime*60){
    HTML(paste0(&amp;quot;Elapsed time: &amp;quot;, ifelse(tm&amp;lt;60, paste0(tm, &amp;quot; seconds.&amp;quot;), 
                                    paste0(tm%/%60, &amp;quot;:&amp;quot;, tm%%60, &amp;quot; mins.&amp;quot;))))
    }else{
      HTML(paste0(&amp;#39;&amp;lt;p style=&amp;quot;color:#9f1317;&amp;quot;&amp;gt;&amp;lt;strong&amp;gt;&amp;#39;,paste0(&amp;quot;Time-up!! (&amp;quot;, ifelse(tm&amp;lt;60, paste0(tm, &amp;quot; seconds).&amp;quot;), 
                                      paste0(tm%/%60, &amp;quot;:&amp;quot;, tm%%60, &amp;quot; mins).&amp;quot;)))),&amp;#39;&amp;lt;/strong&amp;gt;&amp;lt;/p&amp;gt;&amp;#39;)
    }
  })
  
  observeEvent(input$reset, {
    shinyjs::reset(&amp;quot;qqt&amp;quot;)
  })
  
  observeEvent(input$reset_button, {js$reset()})  
  
  observeEvent(input$submit, {
    tm &amp;lt;- as.numeric(round(difftime(
      Sys.time(), EventTime, units = &amp;#39;secs&amp;#39;)))
    
    nq &amp;lt;- substr(as.character(qqt()), start = 1, 
           stop = gregexpr(pattern = &amp;quot;:&amp;quot;, qqt())[[1]][1]-1)
    
    dtm &amp;lt;- data.frame(&amp;quot;School&amp;quot; = input$school, 
                       &amp;quot;Seconds&amp;quot; = tm,
                       &amp;quot;Question&amp;quot; = qqt(),
                       &amp;quot;Number&amp;quot; = nq,
                       &amp;quot;Fluid&amp;quot; = input$score1, 
                       &amp;quot;Content&amp;quot; = input$score2,
                      &amp;quot;Hour&amp;quot; = Sys.time(),
                      &amp;quot;Date&amp;quot; = Sys.Date(),
                      &amp;quot;Type&amp;quot; = &amp;quot;New&amp;quot;)
    
    nm &amp;lt;- paste0(Sys.Date(),
                 gsub(pattern = &amp;quot;:&amp;quot;,
                      replacement = &amp;quot;_&amp;quot;,
                      x = substr(Sys.time(),12,19)))
    
    write.table(dtm, file = paste0(&amp;quot;Interviews_&amp;quot;, nm, &amp;quot;.txt&amp;quot;), 
                sep = &amp;quot;|&amp;quot;, row.names = FALSE)
  })
  
})&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;uir&quot;&gt;ui.R&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(shiny)
library(shinyjs)
jsResetCode &amp;lt;- &amp;quot;shinyjs.reset = function() {history.go(0)}&amp;quot;

shinyUI(fluidPage(
  useShinyjs(),
  # Application title
  titlePanel(&amp;quot;MBA Interview Questions&amp;quot;),

  # Sidebar with a slider input for number of bins
  sidebarLayout(
    sidebarPanel(
      sliderInput(&amp;quot;NormalTime&amp;quot;, &amp;quot;Minutes per response:&amp;quot;,
                  min = 0, max = 60, value = 2
      ),
      selectInput(&amp;quot;response&amp;quot;, &amp;quot;Question type:&amp;quot;,
                  c(&amp;quot;To applicant&amp;quot;,&amp;quot;To interviewer&amp;quot;)),
          # you can select which schools here...        
      selectInput(&amp;quot;school&amp;quot;, &amp;quot;School:&amp;quot;,
                  c(&amp;quot;School 1&amp;quot;,&amp;quot;School 2&amp;quot;)),
      HTML(&amp;#39;&amp;lt;h4&amp;gt; Typical Methods for Behavioural Questions &amp;lt;/h4&amp;gt; &amp;lt;br/&amp;gt;
           &amp;lt;h4&amp;gt; &amp;lt;strong&amp;gt; STAR: &amp;lt;/strong&amp;gt;&amp;lt;h4&amp;gt; &amp;lt;br/&amp;gt;
          &amp;lt;strong&amp;gt;S&amp;lt;/strong&amp;gt;ituation: indentify the situation &amp;lt;br/&amp;gt;
          &amp;lt;strong&amp;gt;T&amp;lt;/strong&amp;gt;ask: identify task or project &amp;lt;br/&amp;gt;
          &amp;lt;strong&amp;gt;A&amp;lt;/strong&amp;gt;ction: describe what you did &amp;lt;br/&amp;gt;
          &amp;lt;strong&amp;gt;R&amp;lt;/strong&amp;gt;esult: summarize the results &amp;lt;br/&amp;gt;
          &amp;lt;h4&amp;gt; &amp;lt;strong&amp;gt; CAR: &amp;lt;/strong&amp;gt;&amp;lt;h4&amp;gt; &amp;lt;/ br&amp;gt;
          &amp;lt;strong&amp;gt;C&amp;lt;/strong&amp;gt;ontext: describe the context &amp;lt;br/&amp;gt;
           &amp;lt;strong&amp;gt;A&amp;lt;/strong&amp;gt;ction: describe what you did &amp;lt;br/&amp;gt;
           &amp;lt;strong&amp;gt;R&amp;lt;/strong&amp;gt;esult: summarize the results &amp;lt;br/&amp;gt;
           &amp;#39;)
    ),
    mainPanel(
      HTML(&amp;#39;&amp;lt;h1&amp;gt; Question &amp;lt;/h1&amp;gt;&amp;#39;),
      hr(),
      h2(textOutput(&amp;#39;qqt2&amp;#39;)),
      uiOutput(&amp;#39;eventTimeRemaining&amp;#39;),
      hr(),
      sliderInput(&amp;quot;score1&amp;quot;, &amp;quot;Fluidity and Security:&amp;quot;,
                  min = 0, max = 10, value = 5
      ),
      sliderInput(&amp;quot;score2&amp;quot;, &amp;quot;Content:&amp;quot;,
                  min = 0, max = 10, value = 5
      ),
      extendShinyjs(text = jsResetCode),
      actionButton(&amp;quot;reset_button&amp;quot;, &amp;quot;New Question&amp;quot;),
      actionButton(&amp;quot;submit&amp;quot;, &amp;quot;Score&amp;quot;)
    )
  )
))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;import-to-analyze&quot;&gt;Import to analyze&lt;/h2&gt;

&lt;p&gt;This is the code used to import all of the files starting with “Interviews” from the folder.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;LoadManyTxt &amp;lt;- function(regex, separator = &amp;quot;|&amp;quot;){
  a &amp;lt;- list.files()[grepl(pattern = regex,
                          list.files())]
  datos &amp;lt;- NULL
  #import
  for(i in 1:length(a)){
    temp &amp;lt;- as.data.frame(read.delim(
      a[i], sep = separator,
      stringsAsFactors = FALSE))
    
    print(paste0(&amp;quot;Importing... &amp;quot;, a[i]))
    if(length(temp)==6){
      temp$Hora &amp;lt;- NA
      temp$Fecha &amp;lt;- NA
      temp$Tipo &amp;lt;- &amp;quot;Previous&amp;quot;
    }
    datos &amp;lt;- rbind.data.frame(datos, temp)
    print(paste(&amp;quot;Data: &amp;quot;, length(datos[,1]),&amp;quot; rows&amp;quot;))
  }
  return(datos)
}
# when in the folder, just do...
d &amp;lt;- LoadManyTxt(&amp;quot;Interviews_&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It might seem intense, but this definitely saved me a few hours. As for the outcome of the interviews, I was invited to join two of my school choices, so it all worked out! Although, having gone through the experience, I think the old mantra: “be yourself” is probably better advice than using this app, so use at your own risk!&lt;/p&gt;

</description>
				<pubDate>Tue, 20 Dec 2016 00:00:00 +0100</pubDate>
				<link>http://localhost:4000/datascience/mba-shiny/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/mba-shiny/</guid>
			</item>
		
			<item>
				<title>When Trump visits... tweets from his trip to Mexico</title>
					<description>&lt;p&gt;I’m sure many of my fellow Mexicans will remember the historically ill-advised (to say the least) decision by our President to invite Donald Trump for a meeting.&lt;/p&gt;

&lt;p&gt;Talking to some fellow colleagues, we couldn’t help but notice that &lt;strong&gt;maybe&lt;/strong&gt; in another era this decision would have been good policy. The problem, some concluded, was the influence of social media today. In fact, the Trump debacle did cause outcry among leading politica voices online.&lt;/p&gt;

&lt;p&gt;I wanted to investigate this further, and thankfully for me, I’ve been using &lt;strong&gt;R&lt;/strong&gt; to collect tweets from a catalog of leading political personalities in Mexico for a &lt;a href=&quot;http://numbersmart.mx/&quot;&gt;personal business project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is a short descriptive look at what the 65 twitter accounts I’m following tweeted between August 27th and September 5th (the Donald announced his visit on August the 30th). I’m sorry I can’t share the dataset, but you get the idea with the code…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(dplyr)
library(stringr)

# 42 of the 65 accounts tweeted between those dates.
d %&amp;gt;% 
  summarise(&amp;quot;n&amp;quot; = n_distinct(NOMBRE))
#   n
#  42&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We can see how mentions of trump spike just about the time it was announced…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;byhour &amp;lt;- d %&amp;gt;% 
  mutate(&amp;quot;MONTH&amp;quot; = as.numeric(month(T_CREATED)), 
         &amp;quot;DAY&amp;quot; = as.numeric(day(T_CREATED)), 
         &amp;quot;HOUR&amp;quot; = as.numeric(hour(T_CREATED)), 
         &amp;quot;TRUMP_MENTION&amp;quot; = str_count(TXT, pattern = &amp;quot;Trump|TRUMP|trump&amp;quot;)) %&amp;gt;% 
  group_by(MONTH, DAY, HOUR) %&amp;gt;% 
  summarise(&amp;quot;N&amp;quot; = n(), 
            &amp;quot;TRUMP_MENTIONS&amp;quot; = sum(TRUMP_MENTION)) %&amp;gt;%
  mutate(&amp;quot;PCT_MENTIONS&amp;quot; = TRUMP_MENTIONS/N*100) %&amp;gt;%
  arrange(desc(MONTH), desc(DAY), HOUR) %&amp;gt;%
  mutate(&amp;quot;CHART_DATE&amp;quot; = as.POSIXct(paste0(&amp;quot;2016-&amp;quot;,MONTH,&amp;quot;-&amp;quot;,DAY,&amp;quot; &amp;quot;, HOUR, &amp;quot;:00&amp;quot;)))

library(ggplot2)  
library(eem)
ggplot(byhour, 
       aes(x = CHART_DATE, 
           y = PCT_MENTIONS)) + 
        geom_line(colour=eem_colors[1]) + 
        theme_eem()+
        labs(x = &amp;quot;Time&amp;quot;, 
             y = &amp;quot;Trump mentions \n (% of Tweets)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_mex_plot1.png&quot; alt=&quot;Trump tweets by mexican officials, percent&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The peak of mentions (as a percentage of tweets) was September 1st at 6 am (75%). But it terms of amount of tweets, it is much more obvious the outcry was following the anouncement and later visit of the candidate:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot(byhour, 
       aes(x = CHART_DATE, 
           y = TRUMP_MENTIONS)) + 
        geom_line(colour=eem_colors[1]) + 
        theme_eem()+
        labs(x = &amp;quot;Time&amp;quot;, 
             y = &amp;quot;Trump mentions \n (# of Tweets)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_mex_plot2.png&quot; alt=&quot;Trump tweets by mexican officials, total&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can also (sort-of) identify the effect of these influencers tweeting. I’m going to add the followers, which are potential viewers, of each tweet mentioning Trump, by hour.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;byaudience &amp;lt;- d %&amp;gt;% 
  mutate(&amp;quot;MONTH&amp;quot; = as.numeric(month(T_CREATED)), 
         &amp;quot;DAY&amp;quot; = as.numeric(day(T_CREATED)), 
         &amp;quot;HOUR&amp;quot; = as.numeric(hour(T_CREATED)), 
         &amp;quot;TRUMP_MENTION&amp;quot; = str_count(TXT, pattern = &amp;quot;Trump|TRUMP|trump&amp;quot;)) %&amp;gt;% 
  filter(TRUMP_MENTION &amp;gt; 0) %&amp;gt;%
  group_by(MONTH, DAY, HOUR) %&amp;gt;% 
  summarise(&amp;quot;TWEETS&amp;quot; = n(), 
            &amp;quot;AUDIENCE&amp;quot; = sum(U_FOLLOWERS)) %&amp;gt;%
  arrange(desc(MONTH), desc(DAY), HOUR) %&amp;gt;%
  mutate(&amp;quot;CHART_DATE&amp;quot; = as.POSIXct(paste0(&amp;quot;2016-&amp;quot;,MONTH,&amp;quot;-&amp;quot;,DAY,&amp;quot; &amp;quot;, HOUR, &amp;quot;:00&amp;quot;)))


ggplot(byaudience, 
       aes(x = CHART_DATE, 
           y = AUDIENCE)) + 
        geom_line(colour=eem_colors[1]) + 
        theme_eem()+
        labs(x = &amp;quot;Time&amp;quot;, 
             y = &amp;quot;Potential audience \n (# of followers)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_mex_plot4.png&quot; alt=&quot;Total audience of trump tweets&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So clearly, I’m stating the obvious. People were talking. But how was the conversation being developed? Let’s first see the type of tweets (RT’s vs drafted individually):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;bytype &amp;lt;- d %&amp;gt;% 
  mutate(&amp;quot;TRUMP_MENTION&amp;quot; = str_count(TXT, pattern = &amp;quot;Trump|TRUMP|trump&amp;quot;)) %&amp;gt;%
  # only the tweets that mention trump
  filter(TRUMP_MENTION&amp;gt;0) %&amp;gt;%
  group_by(T_ISRT) %&amp;gt;% 
  summarise(&amp;quot;count&amp;quot; = n())
kable(bytype)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;T_ISRT&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;count&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FALSE&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;313&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;TRUE&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;164&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;About 1 in 3 was a RT. Comparing to the overall tweets, (1389 out of 3833) this seems not too much of a difference, so it wasn’t necesarrily an influencer pushing the discourse. In terms of the most mentioned by tweet it was our President on the spotlight:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;bymentionchain &amp;lt;- d %&amp;gt;% 
  mutate(&amp;quot;TRUMP_MENTION&amp;quot; = str_count(TXT, pattern = &amp;quot;Trump|TRUMP|trump&amp;quot;)) %&amp;gt;%
  # only the tweets that mention trump
  group_by(TRUMP_MENTION, MENTION_CHAIN) %&amp;gt;% 
  summarise(&amp;quot;count&amp;quot; = n()) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(&amp;quot;GROUPED_CHAIN&amp;quot; = ifelse(grepl(pattern = &amp;quot;EPN&amp;quot;, 
                                        x = MENTION_CHAIN), 
                                  &amp;quot;EPN&amp;quot;, MENTION_CHAIN)) %&amp;gt;% 
  mutate(&amp;quot;GROUPED_CHAIN&amp;quot; = ifelse(grepl(pattern = &amp;quot;realDonaldTrump&amp;quot;, 
                                        x = MENTION_CHAIN), 
                                  &amp;quot;realDonaldTrump&amp;quot;, GROUPED_CHAIN))
                                  
ggplot(order_axis(bymentionchain %&amp;gt;% 
                    filter(count&amp;gt;10 &amp;amp; GROUPED_CHAIN!=&amp;quot;ND&amp;quot;), 
                  axis = GROUPED_CHAIN, 
                  column = count), 
       aes(x = GROUPED_CHAIN_o, 
           y = count)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  theme_eem() + 
  labs(x = &amp;quot;Mention chain \n (separated by _|.|_ )&amp;quot;, y = &amp;quot;Tweets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_mex_plot3.png&quot; alt=&quot;Mentions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How about the actual persons who tweeted? It seemed like news anchor Joaquin Lopez-Doriga and security analyst Alejandro Hope were the most vocal about the visit (out of the influencers i’m following).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;bytweetstar &amp;lt;- d %&amp;gt;% 
  mutate(&amp;quot;TRUMP_MENTION&amp;quot; = ifelse(str_count(TXT, pattern = &amp;quot;Trump|TRUMP|trump&amp;quot;)&amp;lt;1,0,1)) %&amp;gt;%
  group_by(TRUMP_MENTION, NOMBRE) %&amp;gt;% 
  summarise(&amp;quot;count&amp;quot; = n_distinct(TXT))
## plot with ggplot2&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_mex_plot5.png&quot; alt=&quot;Mentions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also grouped each person by his political affiliation and I found it confirms the notion that the conversation on the eve of the visit, at least among this &lt;strong&gt;very small&lt;/strong&gt; subset of twitter accounts, was driven by those with no party afiliation or in the “PAN” (opposition party).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;byafiliation &amp;lt;- d %&amp;gt;% 
  mutate(&amp;quot;MONTH&amp;quot; = as.numeric(month(T_CREATED)), 
         &amp;quot;DAY&amp;quot; = as.numeric(day(T_CREATED)), 
         &amp;quot;HOUR&amp;quot; = as.numeric(hour(T_CREATED)), 
         &amp;quot;TRUMP_MENTION&amp;quot; = ifelse(str_count(TXT, pattern = &amp;quot;Trump|TRUMP|trump&amp;quot;)&amp;gt;0,1,0)) %&amp;gt;% 
  group_by(MONTH, DAY, HOUR, TRUMP_MENTION, AFILIACION) %&amp;gt;% 
  summarise(&amp;quot;TWEETS&amp;quot; = n()) %&amp;gt;%
  arrange(desc(MONTH), desc(DAY), HOUR) %&amp;gt;%
  mutate(&amp;quot;CHART_DATE&amp;quot; = as.POSIXct(paste0(&amp;quot;2016-&amp;quot;,MONTH,&amp;quot;-&amp;quot;,DAY,&amp;quot; &amp;quot;, HOUR, &amp;quot;:00&amp;quot;)))
  
 ggplot(byafiliation, 
       aes(x = CHART_DATE, 
           y = TWEETS, 
           group = AFILIACION, 
           fill = AFILIACION)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  theme_eem() + 
  scale_fill_eem(20) + 
  facet_grid(TRUMP_MENTION ~.) +
  labs(x = &amp;quot;Time&amp;quot;, y = &amp;quot;Tweets \n (By mention of Trump)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_mex_plot6.png&quot; alt=&quot;Mentions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, It’s interesting to note how there is a small spike of the accounts afiliated with the PRI (party in power) on the day after his visit (Sept. 1st). Maybe they were trying to drive the conversation to another place?&lt;/p&gt;
</description>
				<pubDate>Mon, 26 Sep 2016 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/datascience/trump-mextweets/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/trump-mextweets/</guid>
			</item>
		
			<item>
				<title>New banxicoR package</title>
					<description>&lt;p&gt;The &lt;strong&gt;banxicoR&lt;/strong&gt; package is now available on CRAN. Much like &lt;a href=&quot;https://github.com/Eflores89/inegiR&quot;&gt;inegiR&lt;/a&gt; this package aims to bring official Mexican data easily into R, in this case by scrapping &lt;em&gt;iqy&lt;/em&gt; calls to the SIE (Sistema de Información Económica) webservice of the Bank of Mexico.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;The major difference with inegiR is that the Bank of Mexico does not have an API, so this package basically uses &lt;code&gt;rvest&lt;/code&gt; to scrape the generated html. The package then does what it can to save it in a convenient &lt;code&gt;data.frame&lt;/code&gt; or &lt;code&gt;list&lt;/code&gt; (same as inegiR).&lt;/p&gt;

&lt;p&gt;A few caveats:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I do not control the data definitions at the source (Banxico), so I can’t guarantee continuos use.&lt;/li&gt;
  &lt;li&gt;Finding the ID of each series has proven a manual, slow and tedious process because they are not available in the webpage. I’m trying to contact Banxico about getting a catalog, but basically use at your own risk.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;finding-series-ids&quot;&gt;Finding series ID’s&lt;/h3&gt;

&lt;p&gt;To find a specific series ID, I would recommend going to the &lt;a href=&quot;http://www.banxico.org.mx/SieInternet/&quot;&gt;SIE webpage&lt;/a&gt;, navigating towards the desired indicators and then consulting them via HTML. The column name should be the series id (they are usually in this format: “SF60653”, with two characters followed by numbers). The package includes a &lt;strong&gt;small and non exhaustive&lt;/strong&gt; catalog of series &lt;strong&gt;in spanish&lt;/strong&gt;. You can access this by &lt;code&gt;data(&quot;BanxicoCatalog&quot;)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then you can find some id’s…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(banxicoR)
library(dplyr)
data(&amp;quot;BanxicoCatalog&amp;quot;)

# To see how many id&amp;#39;s by parent subject 
BanxicoCatalog %&amp;gt;% 
  group_by(PARENT) %&amp;gt;% 
  summarise(&amp;quot;Id&amp;#39;s&amp;quot; = n())

# Source: local data frame [4 x 2]

#                       PARENT     Id&amp;#39;s
#                       (chr)     (int)
#  1          Billetes y Monedas    45
#  2 Intermediarios Financierios    37
#  3            Sistemas de Pago    49
#  4              Tipo de Cambio     2

# to bring the specific id of the average duration of 200 peso bills...
BanxicoCatalog %&amp;gt;% 
  filter(PARENT == &amp;quot;Billetes y Monedas&amp;quot;) %&amp;gt;% 
  filter(LEVEL_1 == &amp;quot;Duración promedio del billete&amp;quot;) %&amp;gt;% 
  filter(LEVEL_2 == &amp;quot;200 pesos&amp;quot;) %&amp;gt;% 
  select(ID)

# Source: local data frame [1 x 1]

#      ID
#    (chr)
#1    SM32&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;usage&quot;&gt;Usage&lt;/h3&gt;
&lt;p&gt;Now that you have some id’s to download, we can use the &lt;code&gt;banxico_series&lt;/code&gt; function…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Download the Bank of Mexico international reserves
rsv &amp;lt;- banxico_series(series = &amp;quot;SF110168&amp;quot;)

tail(rsv)
#          DATE SF110168
#    2016-01-01 176321.4
#    2016-02-01 178408.8
#    2016-03-01 179708.0
#    2016-04-01 182118.8
#    2016-05-01 179351.0
#    2016-06-01 178829.9&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you want some other fancy things, you can use the options…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;rsv &amp;lt;- banxico_series(series = &amp;quot;SF110168&amp;quot;, 
                      metadata = TRUE, 
                      verbose = TRUE)
# [1] &amp;quot;Data series: SF110168 downloaded&amp;quot;
# [1] &amp;quot;Data series in monthly frequency&amp;quot;
# [1] &amp;quot;Parsing data with 198 rows&amp;quot;

str(rsv)
#List of 2
# $ MetaData:List of 6
#  ..$ IndicatorName: chr &amp;quot;I. Official Reserve Assets And Other Foreign Currency Assets - A. Official Reserve Assets&amp;quot;
#  ..$ IndicatorId  : chr &amp;quot;SF110168&amp;quot;
#  ..$ Units        : chr &amp;quot;millions of u.s. dollar&amp;quot;
#  ..$ DataType     : chr &amp;quot;market value/price&amp;quot;
#  ..$ Period       : chr &amp;quot;jan 2000 - jun 2016&amp;quot;
#  ..$ Frequency    : chr &amp;quot;monthly&amp;quot;
# $ Data    :&amp;#39;data.frame&amp;#39;:	198 obs. of  2 variables:
#  ..$ Dates   : Date[1:198], format: &amp;quot;2000-01-01&amp;quot; ...
#  ..$ SF110168: num [1:198] 33689 33382 36435 34749 33624 ...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Finally, we can graph this…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(ggplot2)
library(eem) # theme from: https://github.com/Eflores89/eem

ggplot(rsv$Data, 
       aes(x = Dates, y = SF110168))+
       geom_path(colour = eem_colors[1])+
       theme_eem()+
       labs(x = &amp;quot;Dates&amp;quot;, 
            y = paste0(&amp;quot;Reserves in U.S. Dollars \n (&amp;quot;, rsv$MetaData$Units, &amp;quot;)&amp;quot;), 
            title = &amp;quot;Bank of Mexico International Reserves&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;http://enelmargen.org/banxicoR/imgs/banxicoR_1.png&quot; alt=&quot;bank of mexico reserves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This data series is also available at INEGI and can be downloaded with &lt;a href=&quot;https://github.com/Eflores89/inegiR&quot;&gt;inegiR&lt;/a&gt; but Banxico has other interesting series exclusive to them, like financial loans or money in circulation, which I encourage everyone to check out!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/eflores89&quot;&gt;Tweet&lt;/a&gt; me up if you have any suggestions / improvements or open &lt;a href=&quot;https://github.com/Eflores89/banxicoR&quot;&gt;an issue at Github&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 19 Aug 2016 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/banxicoR/vignette-v09/</link>
				<guid isPermaLink="true">http://localhost:4000/banxicoR/vignette-v09/</guid>
			</item>
		
			<item>
				<title>Voting clusters in the U.N.</title>
					<description>&lt;p&gt;After some more digging, and a suggestion by &lt;a href=&quot;https://twitter.com/theMexIndian&quot;&gt;@theMexIndian&lt;/a&gt; I decided to see more in the depth the unvotes database that I &lt;a href=&quot;http://enelmargen.org/datascience/un-voting-patterns/&quot;&gt;wrote about&lt;/a&gt; some weeks ago.&lt;/p&gt;

&lt;p&gt;This time, amit suggested I do some hierarchical clustering of the votes. So here goes a &lt;strong&gt;very dirty&lt;/strong&gt; first attempt…&lt;/p&gt;

&lt;h2 id=&quot;data-and-setup&quot;&gt;Data and setup&lt;/h2&gt;

&lt;p&gt;Nothing too impressive here… (for a discussion of the package, see the original post).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(dplyr)
library(magrittr)
library(unvotes)
library(reshape2)

# number of roll-calls
votes &amp;lt;- un_votes %&amp;gt;%
  left_join(., un_roll_calls) %&amp;gt;%
  left_join(., un_roll_call_issues)

length(unique(votes$rcid))
# [1] 5275 # number of unique roll call votes&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;There are more than 5k unique roll calls, so if we where to open up dimensionality by each roll call, this is going to be huge, but i’ll go ahead and do it anyways, to test a hypothesis towards the end…&lt;/p&gt;

&lt;h2 id=&quot;widen-data&quot;&gt;‘Widen’ data…&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;wide &amp;lt;- votes %&amp;gt;% 
  select(rcid, country, vote) %&amp;gt;% 
  dcast(, formula = rcid+country ~ vote) %&amp;gt;% 
  dcast(, formula = country~rcid+yes+no+abstain)

str(wide)
# &amp;#39;data.frame&amp;#39;:	200 obs. of  14352 variables:&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now that we have a very high dimension data set (each variable is the vote in a roll call, for example, abstain_120, yes_120, no_120 would be a count of abstain, yes and no votes in roll call 120). This data set is basically ones and ceros. Now to do some cleaning and get the distance matrix…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;wide[is.na(wide)] &amp;lt;- 0
d_wide &amp;lt;- as.matrix(wide)
row.names(d_wide) &amp;lt;- wide$country # to name rows
d_wide &amp;lt;- dist(d_wide) # distance matrix
hc_wide &amp;lt;- hclust(d_wide) # hierarchical cluster&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s graph this hierarchical clustering using the &lt;code&gt;ggdendro&lt;/code&gt; package…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(ggdendro)
library(eem) # blog colors
ggdendrogram(hc_wide, 
  rotate = TRUE) + 
    theme_eem() + 
    theme(axis.text.y = element_text(size=6)) + 
    labs(x = &amp;quot;country&amp;quot;, 
         y = &amp;quot;&amp;quot;, 
         title = &amp;quot;Hierarchical clusters of votes \n in U.N.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/dendo_1.png&quot; alt=&quot;Dendogram of UN votes by country&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m going to export these clusters and upload them on &lt;a href=&quot;https://github.com/Eflores89/proyectos/tree/master/data&quot;&gt;my github&lt;/a&gt; for anyone to download.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;hc_c &amp;lt;- cutree(hc_wide, k = 8)
hc_c &amp;lt;- as.data.frame(hc_c, row.names = names(hc_c))
hc_c$c &amp;lt;- row.names(hc_c)
cc &amp;lt;- hc_c %&amp;gt;% arrange(-hc_c)

write.csv(as.data.frame(cc), file = &amp;quot;country_clusters.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;by-issues&quot;&gt;By issues&lt;/h2&gt;

&lt;p&gt;Now, because the latest data set was very high dimension, i’m going to condense the analysis to just votes on particular issues. The data base has seven core issues, so i’m going to try to group by issue instead of roll call. This might let us see if there are different voting blocs from the earlier set (maybe countries vote the same, except when important issues come up).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Widen, by issue...
wide_byissue &amp;lt;- votes %&amp;gt;% 
  select(issue, country, vote) %&amp;gt;% 
  dcast(, formula = country ~ vote+issue)

wide_byissue[is.na(wide_byissue)] &amp;lt;- 0
d_wide_issue &amp;lt;- as.matrix(wide_byissue)
row.names(d_wide_issue) &amp;lt;- wide_byissue$country
d_wide_issue &amp;lt;- dist(d_wide_issue)
hc_wide_issue &amp;lt;- hclust(d_wide_issue)

ggdendrogram(hc_wide_issue, 
  rotate = TRUE) + 
    theme_eem() + 
    theme(axis.text.y = element_text(size=6)) + 
    labs(x = &amp;quot;country&amp;quot;, 
         y = &amp;quot;&amp;quot;, 
         title = &amp;quot;Hierarchical clusters of votes \n in U.N. (issues)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/dendo_2.png&quot; alt=&quot;Dendogram of UN votes by country, when grouping by issue&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ll export this too…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;hc_c2 &amp;lt;- cutree(hc_wide_issue, k = 8)
hc_c2 &amp;lt;- as.data.frame(hc_c2, row.names = names(hc_c2))
hc_c2$c &amp;lt;- row.names(hc_c2)
cc2 &amp;lt;- hc_c2 %&amp;gt;% arrange(-hc_c2)

write.csv(as.data.frame(cc2), file = &amp;quot;country_clusters_issue.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To disprove the earlier hypothesis, i’m going to find Mexico’s neighborhood, and see if there are many countries that repeat themselves in both sets…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# find cluster where Mexico lives ... 
neighborhood_mx &amp;lt;- hc_c %&amp;gt;% filter(hc_c == 3)
neighborhood_mx_issue &amp;lt;- hc_c2 %&amp;gt;% filter(hc_c2 == 1)

sum(neighborhood_mx_issue$c %in% neighborhood_mx$c)/length(neighborhood_mx_issue$c)
# [1] 0.8

# export mexico&amp;#39;s neighborhood
write.csv(neighborhood_mx_issue, file = &amp;quot;neighborhood_mx_issue.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;So 80% of the country’s are “close” to Mexico whether the vote is by issue or by roll call. This is a rough first attempt (there are probably many slight errors) but there are some interesting things to be found.&lt;/p&gt;

&lt;p&gt;In the issues groups, the outliers in a single group are the United States and Israel (the Palestinian conflict probably is the culprit here, as I found earlier, they agree on 77% of the votes).&lt;/p&gt;

&lt;p&gt;Then there are countries that seem to be very close culturally, and they show it in the votes…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# advanced foreign policy
hc_c2 %&amp;gt;% filter(hc_c2 == &amp;quot;6&amp;quot;)
# [1] &amp;quot;Austria&amp;quot;     &amp;quot;Denmark&amp;quot;     &amp;quot;Finland&amp;quot;     &amp;quot;Greece&amp;quot;      &amp;quot;Iceland&amp;quot;   
# [6] &amp;quot;Ireland&amp;quot;     &amp;quot;Japan&amp;quot;       &amp;quot;New Zealand&amp;quot; &amp;quot;Norway&amp;quot;      &amp;quot;Spain&amp;quot;      
# [11] &amp;quot;Sweden&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Finally, some like-minded countries, like Chile, Colombia, Panama, Paraguay, Peru, etc are in Mexico’s neighborhood (although it’s one of the largest groups).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/eflores89&quot;&gt;Tweet&lt;/a&gt; me up if you have any questions with the data!&lt;/p&gt;
</description>
				<pubDate>Wed, 27 Jul 2016 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/datascience/un-voting-communities/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/un-voting-communities/</guid>
			</item>
		
			<item>
				<title>Analyzing Mexican votes in the U.N.</title>
					<description>&lt;p&gt;Last week I discovered the &lt;a href=&quot;https://github.com/dgrtwo/unvotes&quot;&gt;unvotes&lt;/a&gt; package on github so I thought i’d do some number-crunching to see if I find anything interesting.&lt;/p&gt;

&lt;p&gt;The package provides the voting history of countries in the United Nations General Assembly, along with information such as date, description, and topics for each vote.&lt;/p&gt;

&lt;p&gt;The author makes it clear to reference the original publication of the data:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Erik Voeten “Data and Analyses of Voting in the UN General Assembly” Routledge Handbook of International Organization, edited by Bob Reinalda (published May 27, 2013)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I installed from CRAN and created a small function that counts the amount of times two countries “agree”, that is they vote the same way in a given resolution.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(dplyr)
library(magrittr)
library(unvotes)

# the database of all votes 
votes &amp;lt;- un_votes %&amp;gt;%
  left_join(., un_roll_calls) %&amp;gt;%
  left_join(., un_roll_call_issues)

# a function to count the amount of times any given countries &amp;quot;agree&amp;quot;
un_agree &amp;lt;- function(db, # the database
                     country1, 
                     country2,
                     abstain = TRUE # if FALSE, do not count abstains
                     ){
  c1 &amp;lt;- db %&amp;gt;% 
    filter(country == country1) %&amp;gt;%
    mutate(&amp;quot;vote2&amp;quot; = vote)
  c2 &amp;lt;- db %&amp;gt;% 
    filter(country == country2)
  j &amp;lt;- c1 %&amp;gt;% select(vote2, rcid) %&amp;gt;%
    left_join(., c2 %&amp;gt;% select(rcid, vote), by = &amp;quot;rcid&amp;quot;) %&amp;gt;%
    filter(!is.na(vote))
  if(!abstain){
  j &amp;lt;- j %&amp;gt;% 
    filter(vote != &amp;quot;abstain&amp;quot;) %&amp;gt;%
    filter(vote2 != &amp;quot;abstain&amp;quot;)
  }
  j &amp;lt;- j %&amp;gt;% 
    mutate(&amp;quot;agree&amp;quot; = ifelse(vote2 == as.character(vote), 1, 0))
  sum(j$agree)/length(j$agree)*100
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now, it would be interesting to see how much my home country and its rowdy neighbour to the north see eye-to-eye…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;un_agree(votes, &amp;quot;Mexico&amp;quot;, &amp;quot;United States&amp;quot;)
# [1] 23.32385&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;So about one-fourth of every vote. This goes up slightly when eliminating abstentions (29.7%).&lt;/p&gt;

&lt;p&gt;However, this might be interesting to see by “sexenio”, or Mexican presidential term. Small caveat, the last vote in the dataset is september 9th, 2014.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# some of the newest presidential terms...
votes %&amp;lt;&amp;gt;% mutate(&amp;quot;mxterm&amp;quot; = ifelse(date &amp;gt;= &amp;#39;1988-12-01&amp;#39; &amp;amp; 
                                   date &amp;lt;= &amp;#39;1994-11-30&amp;#39;, &amp;quot;salinas&amp;quot;, 
                            ifelse(date &amp;gt;= &amp;#39;1994-12-01&amp;#39; &amp;amp; 
                                   date &amp;lt;= &amp;#39;2000-11-30&amp;#39;, &amp;quot;zedillo&amp;quot;,
                            ifelse(date &amp;gt;= &amp;#39;2000-12-01&amp;#39; &amp;amp; 
                                   date &amp;lt;= &amp;#39;2006-11-30&amp;#39;, &amp;quot;fox&amp;quot;,
                            ifelse(date &amp;gt;= &amp;#39;2006-12-01&amp;#39; &amp;amp; 
                                   date &amp;lt;= &amp;#39;2012-11-30&amp;#39;, &amp;quot;calderon&amp;quot;,
                            ifelse(date &amp;gt;= &amp;#39;2012-12-01&amp;#39; &amp;amp; 
                                   date &amp;lt;= &amp;#39;2018-11-30&amp;#39;, &amp;quot;penanieto&amp;quot;, &amp;quot;others&amp;quot;))
                                   ))))
un_agree(votes %&amp;gt;% filter(mxterm ==&amp;quot;salinas&amp;quot;), 
        &amp;quot;Mexico&amp;quot;, &amp;quot;United States&amp;quot;)
# [1] 10.77586
un_agree(votes %&amp;gt;% filter(mxterm ==&amp;quot;zedillo&amp;quot;), 
        &amp;quot;Mexico&amp;quot;, &amp;quot;United States&amp;quot;)
# [1] 20.76167
un_agree(votes %&amp;gt;% filter(mxterm ==&amp;quot;fox&amp;quot;), 
        &amp;quot;Mexico&amp;quot;, &amp;quot;United States&amp;quot;)
# [1] 9.129213
un_agree(votes %&amp;gt;% filter(mxterm ==&amp;quot;calderon&amp;quot;), 
        &amp;quot;Mexico&amp;quot;, &amp;quot;United States&amp;quot;)
# [1] 8.935743
un_agree(votes %&amp;gt;% filter(mxterm ==&amp;quot;penanieto&amp;quot;), 
        &amp;quot;Mexico&amp;quot;, &amp;quot;United States&amp;quot;)
# [1] 12.92517&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This is unexpected. It’s usually folk tale that the PAN-ista governments (Fox and especially Calderon) cooperated much more with their American counterparts than the rest. Maybe we could visualize this side by side in a graph:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;agree_by_year &amp;lt;- votes %&amp;gt;% 
  filter(country %in% c(&amp;quot;Mexico&amp;quot;, &amp;quot;United States&amp;quot;)) %&amp;gt;% 
  group_by(year = year(date), rcid) %&amp;gt;% 
  summarise(&amp;quot;DifVote&amp;quot; = n_distinct(vote)) %&amp;gt;% 
  mutate(&amp;quot;Agree&amp;quot; = ifelse(DifVote == 1, 1, 0)) %&amp;gt;% 
  summarise(&amp;quot;PercentageAgree&amp;quot; = sum(Agree)/n_distinct(rcid), 
            &amp;quot;VoteCount&amp;quot; = n_distinct(rcid))
  
g &amp;lt;- ggplot(agree_by_year, 
        aes(x = year, 
            y = PercentageAgree*100))+
  geom_line(colour = eem_colors[1])+
  theme_eem()+
  labs(title = &amp;quot;Do Mexico and U.S. agree in U.N. votes?&amp;quot;,
       x = &amp;quot;Year&amp;quot;, y = &amp;quot;(%)&amp;quot;)
g&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/unvotes2.png&quot; alt=&quot;Mexico and the US agree less on UN votes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So the tendency has been to disagree more under these presidents. Off course, this could be due to a number of quirky data issues. Among them, the amount of different votes taken to the General Assembly (in orange)…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;g +
  geom_line(aes(x = year, y = VoteCount), colour = eem_colors[2])&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/unvotes3.png&quot; alt=&quot;Votes by year where Mexico and USA voted in UN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, it might seem strange so much disagreement, so let’s see if this rate is small or large in the context of everyone else. I’m going to loop through every country, to see Mexico’s (historic) agreement with each one…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# I found making a function to be easier for the rest
# of the post... 
agree_world_with &amp;lt;- function(db, 
                             country_pivot){
v &amp;lt;- NULL
for(i in 1:length(unique(db$country))){
country &amp;lt;- unique(db$country)[i]
p &amp;lt;- un_agree(db = votes, 
         country1 = country_pivot, 
         country2 = unique(db$country)[i])
d &amp;lt;- data.frame(&amp;quot;country&amp;quot; = country, &amp;quot;p&amp;quot; = p)
v &amp;lt;- rbind.data.frame(v, d)}
  return(v)
}

# Now for the fun part... 
# which countries agree most with Mexico?
agree_world_with(votes, &amp;quot;Mexico&amp;quot;) %&amp;gt;% 
  arrange(desc(p)) %&amp;gt;%
  filter(country != &amp;quot;Mexico&amp;quot;) %&amp;gt;% # obviously p=100
  head()
#                country        p
#1           Timor-Leste 95.82192
#2            Seychelles 94.46378
#3 Sao Tome and Principe 93.73695
#4         Guinea-Bissau 92.48287
#5              Zimbabwe 92.37548
#6              Suriname 92.37288

# which countries agree less with Mexico?
agree_world_with(votes, &amp;quot;Mexico&amp;quot;) %&amp;gt;% 
  arrange(desc(p)) %&amp;gt;%
  filter(country != &amp;quot;Mexico&amp;quot;) %&amp;gt;%
  tail()
#                            country        p
#194                Marshall Islands 35.27607
#195 Micronesia, Federated States of 27.76163
#196                          Israel 25.96885
#197                   United States 23.32385
#198                           Palau 20.79327
#199                        Zanzibar  0.00000&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For the record, I tried doing the same exercise for each “sexenio” and basically came up with the same number of countries on top, maybe we’re not so close to the U.S. after all?&lt;/p&gt;

&lt;p&gt;What about abstentions? You would think Mexico’s historical no-intervention foreign policy would make it a cronical abstainer, but actually even the United States and France use this trick more often…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# count abstentions by country
abstentions &amp;lt;- votes %&amp;gt;% 
  mutate(&amp;quot;abstain&amp;quot; = ifelse(as.character(vote) == &amp;quot;abstain&amp;quot;, 1, 0)) %&amp;gt;%
  group_by(country) %&amp;gt;% 
  summarise(&amp;quot;prct_abstentions&amp;quot; = sum(abstain)/n_distinct(rcid)) %&amp;gt;%
  arrange(desc(prct_abstentions))
abstentions[1:30, ] %&amp;gt;% as.data.frame
#                           country prct_abstentions
#1                         Zanzibar        1.0000000
#2      Federal Republic of Germany        0.5183636
#3                           France        0.4210017
#4                            Tonga        0.4038710
#5                      South Sudan        0.3958333
#6                            Japan        0.3929402
#7                          Georgia        0.3845050
#8                            Italy        0.3822297
#9  Micronesia, Federated States of        0.3818046
#10                         Austria        0.3641872
#11                  United Kingdom        0.3614412
#12              Korea, Republic of        0.3612663
#13                         Belgium        0.3587247
#14                      Luxembourg        0.3495841
#15                        Portugal        0.3486631
#16                          Canada        0.3462857
#17                Marshall Islands        0.3426431
#18                     Netherlands        0.3395579
#19                         Ireland        0.3381340
#20                         Finland        0.3379368
#21                       Australia        0.3326978
#22            Moldova, Republic of        0.3320053
#23                          Sweden        0.3277905
#24                          Israel        0.3275574
#25                         Denmark        0.3267912
#26                          Norway        0.3253862
#27                         Iceland        0.3208144
#28                         Estonia        0.3169192
#29                          Latvia        0.3159236
#30                           Spain        0.3145144

# Mexico is actually at spot 122 with only 10% of its total votes!! 
abstentions[122, ]
#  1  Mexico        0.1094109&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This dataset is actually really nice, we can make a small comparison of some foreign policy. For example NAFTA partners and their agreement on some issues related to Israel…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# only the Palestinian conflict
israelset &amp;lt;- votes %&amp;gt;% filter(issue == &amp;quot;Palestinian conflict&amp;quot;)
nafta &amp;lt;- c(&amp;quot;Mexico&amp;quot;, &amp;quot;United States&amp;quot;, &amp;quot;Canada&amp;quot;)
agree_with_israel &amp;lt;- data.frame(&amp;quot;country&amp;quot; = nafta, 
                                &amp;quot;agree&amp;quot; = c(
                                un_agree(israelset, nafta[1], &amp;quot;Israel&amp;quot;),
                                un_agree(israelset, nafta[2], &amp;quot;Israel&amp;quot;),
                                un_agree(israelset, nafta[3], &amp;quot;Israel&amp;quot;)
                                ))
agree_with_israel
#        country     agree
#1        Mexico  5.156724
#2 United States 77.620968
#3        Canada 25.226131&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I’m sure the kind readers at &lt;strong&gt;r-bloggers&lt;/strong&gt; will come up with another useful analysis from this package. Maybe a map? &lt;a href=&quot;https://twitter.com/eflores89&quot;&gt;Tweet&lt;/a&gt; me up!&lt;/p&gt;
</description>
				<pubDate>Tue, 12 Jul 2016 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/datascience/un-voting-patterns/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/un-voting-patterns/</guid>
			</item>
		
			<item>
				<title>satRday in Monterrey</title>
					<description>&lt;p&gt;I recently volunteered to host a satRday event in Monterrey through the email form and was surprised to find out it was shortlisted as an international destination. I’m writing this to convince others to vote for Monterrey.&lt;/p&gt;

&lt;p&gt;If you’re nearby, like the north of Mexico, the benefits are clear (the next option is currently South Africa!).&lt;/p&gt;

&lt;h2 id=&quot;overview-about-monterrey&quot;&gt;Overview about Monterrey&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Monterrey&quot;&gt;Monterrey&lt;/a&gt; is one of the largest cities in Mexico and arguably the richest in per capita terms.&lt;/p&gt;

&lt;p&gt;Traditionally, the city is a business, manufacturing and educational hub. A number of very large multinationals have their global or regional corporate offices here (Coca-Cola Femsa, Arca, Axtel, Cemex, Office Depot, Ternium, Heineken, Soriana, Banorte, Alfa, etc).&lt;/p&gt;

&lt;p&gt;Conectivity with the U.S. economy is stronger than with the rest of the country, meaning that growth and productivity have generally been closer to our partners in the north than the rest of the country. Monterrey is also known for its entrepreneurial spirit, which is why it also boasts a number of innovation initiatives by government and private sector. The financial sector is also considerably advanced in comparison to the rest of the region.&lt;/p&gt;

&lt;p&gt;Education is also very big. Tec de Monterrey, one of the best universities in Latin America (and the world) is based in Monterrey, as well as UANL which is the second best and largest public University in the country. UDEM and UR also boast high levels of educational awards and accreditations. Thousands of students flock to these universities from the rest of the country every year.&lt;/p&gt;

&lt;p&gt;Finally, the airport is extremely well connected, with more than 350 flights a day from destinations in the U.S, Mexico and even transcontinental direct flights (Japan and Europe). Flying via Mexico City is also extremely cheap and convenient (there are about 15 two-way flights a day between both cities). In 2015, the airport handled 8.4 million passengers, a 18.7% increase from 2014 (somewhere between Tampa and San Diego airports’ capacities). There also tons and tons of hotels and convention centers…&lt;/p&gt;

&lt;p&gt;So, I would argue that if your based in Texas, a &lt;strong&gt;satRday&lt;/strong&gt; in Monterrey is closer to you than one in Chicago or D.C. Plus, you get to explore a new, wonderful city:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/mty1.jpg&quot; alt=&quot;Foto1 monterrey&quot; height=&quot;80%&quot; width=&quot;80%&quot; /&gt;
&lt;img src=&quot;/images/posts/mty2.jpg&quot; alt=&quot;Foto2 monterrey&quot; height=&quot;80%&quot; width=&quot;80%&quot; /&gt;
&lt;img src=&quot;/images/posts/mty3.jpg&quot; alt=&quot;Foto3 monterrey&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vote for Monterrey&lt;/strong&gt; here: &lt;a href=&quot;http://app.doopoll.co/poll/ZznsEGPnmbFafim2c&quot;&gt;http://app.doopoll.co/poll/ZznsEGPnmbFafim2c&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 14 May 2016 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/datascience/mty-satrday/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/mty-satrday/</guid>
			</item>
		
			<item>
				<title>New nasadata R package</title>
					<description>&lt;p&gt;This package intends to provide a hassle-free way to access some of NASA’s open-source API’s to build applications or models.&lt;/p&gt;

&lt;p&gt;Because the documentation seems inconsistent and there are &lt;em&gt;tons&lt;/em&gt; of API’s, I have concentrated my efforts on three which I believe provide the best “bang for my money”.&lt;/p&gt;

&lt;p&gt;The source package is built around these three API’s, but for the sake of clarity, i’ll group the first two into one example.&lt;/p&gt;

&lt;h2 id=&quot;eonet-webservice&quot;&gt;EONET Webservice&lt;/h2&gt;

&lt;p&gt;The Earth Observatory Natural Event Tracker is a Webservice that feeds curated natural “events” that are tracked by a few sources. It is somewhat unclear what exactly constitutes an “event”…&lt;/p&gt;

&lt;p&gt;From the &lt;a href=&quot;http://eonet.sci.gsfc.nasa.gov/eonet-project&quot;&gt;official docs&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The curation of events is a big component of the EONET system and while the technical details are, to an extent, straight forward, the definition of what exactly constitutes an event is fluid and daring us to be constrained. What are the contextual parameters of an event? If one curator defines a specific wildfire in Idaho as a discrete event and another defines the summer wildfire season in the Pacific Northwest as a single event, what does that mean for the end user? If an end user can filter by source/curator does that provide them with ample context for the development of their application?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;We are still thinking about these issues and how to best represent them within EONET and we encourage you to get in touch with us if you have ideas or suggestions or use cases that you have developed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, at least this gives us an idea. An event is something like a storm, fire, drought, etc.&lt;/p&gt;

&lt;p&gt;So, let’s imagine we want to see some action that occured a few days ago.&lt;/p&gt;

&lt;p&gt;I’ll start with installing the package:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;Eflores89/nasadata&amp;quot;)
library(nasadata)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now, what &lt;strong&gt;kind&lt;/strong&gt; of event do I want? We can query the webservice to find all of the available ones like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;categories &amp;lt;- eonet_categories()
  # there are a few columns, I&amp;#39;ll only show two...
names(categories)
# [1] &amp;quot;id&amp;quot;          &amp;quot;title&amp;quot;       &amp;quot;link&amp;quot;        &amp;quot;description&amp;quot; &amp;quot;layers&amp;quot; 
categories[,1:2]
#   id                title
#   6              Drought
#   7        Dust and Haze
#  16          Earthquakes
#   9               Floods
#  14           Landslides
#  19              Manmade
#  15     Sea and Lake Ice
#  10        Severe Storms
#  17                 Snow
#  18 Temperature Extremes
#  12            Volcanoes
#  13          Water Color
#  8            Wildfires&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Who is reporting these events is probably also important, so we can see this with a similar function:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;sources &amp;lt;- eonet_sources()
  # there are a few columns, I&amp;#39;ll only show two...
names(sources)
# [1] &amp;quot;id&amp;quot;     &amp;quot;title&amp;quot;  &amp;quot;source&amp;quot; &amp;quot;link&amp;quot; 
sources[,1:2]
#        id                                                 title
#    CALFIRE California Department of Forestry and Fire Protection
#       CEMS               Copernicus Emergency Management Service
#         EO                                     Earth Observatory
#      GDACS         Global Disaster Alert and Coordination System
#      GLIDE                      GLobal IDEntifier Number (GLIDE)
#    InciWeb                                               InciWeb
#        IDC    International Charter on Space and Major Disasters
#        MRR                                  LANCE Rapid Response
#  NASA_ESRS            NASA Earth Science and Remote Sensing Unit
#  ReliefWeb                                             ReliefWeb
#  SIVolcano        Smithsonian Institute Global Volcanism Program
#     UNISYS                                        Unisys Weather
#   USGS_CMT  USGS Emergency Operations Collection Management Tool
#       HDDS                 USGS Hazards Data Distribution System&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now that we got this out of the way, let’s see if we can get an individual event. The &lt;code&gt;earth_event()&lt;/code&gt; function does this in an intuitive way. I’m going to bring only the latest event reported by InciWeb:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;an_event &amp;lt;- earth_event(status = &amp;quot;all&amp;quot;, 
                        sources = &amp;quot;InciWeb&amp;quot;, 
                        category_id = &amp;quot;all&amp;quot;, 
                        limit = 1,
                        LimitType = &amp;quot;limit&amp;quot;)
class(an_event)
# [1] &amp;quot;list&amp;quot;
names(an_event)
# &amp;quot;Events&amp;quot;     &amp;quot;Sources&amp;quot;    &amp;quot;Categories&amp;quot; &amp;quot;Geography&amp;quot;  &amp;quot;Meta&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The event is a list with a few objects parsed together:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Events:&lt;/strong&gt; Gives us an overview of each event(s) in a data.frame. This includes id, title, description, link.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sources:&lt;/strong&gt; Tells us the sources by event id in a data.frame.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Categories:&lt;/strong&gt; Categories by event id (also in a data.frame).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Geography:&lt;/strong&gt; Gives us the coordinates or polygon where the event took place. This can be a list with lists. For example, if there are several coordinates and times (if it is an event that was prolonged or moved, like a hurricane).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Meta:&lt;/strong&gt; Some metadata related to the query, including the string used.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this case, we have a fire in North Carolina…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;an_event$Events$event_id
# [1] &amp;quot;EONET_382&amp;quot;
an_event$Events$event_title
# [1] &amp;quot;Silver Mine Fire, North Carolina&amp;quot;
# --- We can actually find it here:
an_event$Sources$source_url
# [1] &amp;quot;http://inciweb.nwcg.gov/incident/4706/&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And this is where it happened…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;an_event$Geography
#$EONET_382
#                  date  type         coordinates
#1 2016-04-21T15:00:00Z Point -82.80806, 35.89028&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The coordinates are of course useful because we plot the events in our favorite &lt;strong&gt;R&lt;/strong&gt; package (leaflet or ggmap is really nice) or we can also use it to “feed” into the other API’s in the package. I’ll explain these next…&lt;/p&gt;

&lt;h2 id=&quot;imagery-and-assets-apis&quot;&gt;Imagery and Assets API’s&lt;/h2&gt;

&lt;p&gt;The Earth Imagery API (available &lt;a href=&quot;https://api.nasa.gov/api.html#earth&quot;&gt;here&lt;/a&gt;) let’s us access imagery that is being retrieved from Landsat 8 Satellites and stored in Google Earth Engine. From what I can see, the images are poor quality but the obvious point here is to merge them and detect large variations (for example, to track deforestation).&lt;/p&gt;

&lt;p&gt;The Assets API is basically a helper for the Imagery API: it gives us dates of available coordinates, so that we can query the latter and retrieve the image.&lt;/p&gt;

&lt;p&gt;Let’s see if we can see the “event” we recorded earlier…&lt;/p&gt;

&lt;p&gt;First, I want to see if there is any images recorded in that time frame (the frecuency is roughly every 16 days).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Caveat: This API requieres an active key. It’s free and you can get it at api.nasa.gov&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# coordinates of event
coord_long &amp;lt;- (-82.80806)
coord_lat &amp;lt;- 35.89028
key &amp;lt;- &amp;quot;example1234key&amp;quot;

# images available
images &amp;lt;- earth_asset(key, 
                      lon = coord_long, 
                      lat = coord_lat, 
                      start_date = &amp;quot;2016-03-01&amp;quot;, 
                      end_date = &amp;quot;2016-04-30&amp;quot;)
names(images)
# [1] &amp;quot;date&amp;quot;        &amp;quot;id&amp;quot;          &amp;quot;type&amp;quot;        &amp;quot;coordinates&amp;quot;
images$date
# [1] 2016-03-01T16:05:40 
# [2] 2016-03-17T16:05:35
# [3] 2016-04-02T16:05:26 
# [4] 2016-04-18T16:05:20&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;So, it seems we’re out of luck… The fire ocurred &lt;strong&gt;after&lt;/strong&gt; the last picture taken. Nonetheless, let’s see how we can download this last image…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;img &amp;lt;- earth_image(key, 
                  lon = coord_long, 
                  lat = coord_lat, 
                  date = &amp;quot;2016-04-18&amp;quot;, plot = TRUE)
class(img)
# [1] &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;code&gt;img&lt;/code&gt; object is a list consisting of two objects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;image_data&lt;/strong&gt;: Information about the image (date, url, cloud_score (if parameter is set to TRUE), id)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;image_png&lt;/strong&gt;: Matrix representation of the image (obtained via &lt;code&gt;png::readPNG&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If the parameter is set to TRUE, you also get a rasterImage, in this case, this one…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/nasadata/imgs/earthimage_nasadata.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This package is still in the early stages of development, and I plan on submitting to CRAN in a few weeks, so any suggestions/improvements are very welcome at my twitter: &lt;a href=&quot;https://twitter.com/eflores89&quot;&gt;@eflores89&lt;/a&gt; or via issues in the github package repo: &lt;a href=&quot;https://github.com/Eflores89/nasadata&quot;&gt;https://github.com/Eflores89/nasadata&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sun, 01 May 2016 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/nasadata/vignette_v0/</link>
				<guid isPermaLink="true">http://localhost:4000/nasadata/vignette_v0/</guid>
			</item>
		
			<item>
				<title>inegiR version 1.2</title>
					<description>&lt;p&gt;Version 1.2 of &lt;em&gt;inegiR&lt;/em&gt; is now on &lt;a href=&quot;https://cran.r-project.org/web/packages/inegiR/index.html&quot;&gt;CRAN&lt;/a&gt; so I thought I’d write a few words/vignette about what’s new or different, if at all. By the way, i’m writing in english because more people seem to read &lt;a href=&quot;http://www.r-bloggers.com&quot;&gt;r-bloggers&lt;/a&gt; than my blog (no surprise there), however the pdf manual and &lt;a href=&quot;http://enelmargen.org/inegiR/&quot;&gt;most documentation&lt;/a&gt; is still in spanish.&lt;/p&gt;

&lt;h1 id=&quot;bug-fixes&quot;&gt;Bug fixes&lt;/h1&gt;

&lt;p&gt;Thanks to Diego Valle who reported a &lt;a href=&quot;https://github.com/Eflores89/inegiR/issues/10&quot;&gt;slight bug&lt;/a&gt;, the more random dates (“bienal” and “decenal”) were not being parsed correctly.&lt;/p&gt;

&lt;p&gt;Also added warnings and error handling when the data doesn’t exist for municipalities (issue is &lt;a href=&quot;https://github.com/Eflores89/inegiR/issues/11&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;h1 id=&quot;new-functions&quot;&gt;New functions&lt;/h1&gt;

&lt;h2 id=&quot;grids&quot;&gt;Grids&lt;/h2&gt;

&lt;p&gt;Thanks to Arturo Cardenas who unwittingly built a new function for the DENUE part of the package that’s incorporated in this version.&lt;/p&gt;

&lt;p&gt;As he &lt;a href=&quot;http://arturocm.github.io/blog/r/leaflet/inegir/rtodolist/a-mexican-standoff&quot;&gt;wrote&lt;/a&gt; in his blog, the denue API only allows us to download businesses in a radious of a maximum of 5 kilometers. However, we can get around this limitation by asking the API a series of coordinates that we know overlap each other to create a square of a larger size. This is a picture, taken from that post, detailing what I mean:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/inegiR/imgs/grid_example.png&quot; alt=&quot;example of a grid&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each circle is, of course 5 kms in radius and so the API would give us everything inside.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;hacer_grid()&lt;/code&gt; function helps us in the process by creating a data.frame with a series of coordinates that create a grid like the one in the image if we supply it 2 corners in latitud and longitud.&lt;/p&gt;

&lt;p&gt;But the more powerful &lt;code&gt;denue_grid()&lt;/code&gt; does the interesting part. Using the former function, it also downloads the denue data and returns a unique business data.frame in that grid (if you want duplicates as well, you can eliminate the unique part by setting the &lt;code&gt;unicos = FALSE&lt;/code&gt; parameter)&lt;/p&gt;

&lt;h2 id=&quot;example-with-grids&quot;&gt;Example with Grids&lt;/h2&gt;

&lt;p&gt;Here is an example with the city of Monterrey, let’s say I want all the businesses in San Pedro (a municipality that is part of the metropolitan area).&lt;/p&gt;

&lt;p&gt;The total area is roughly about 45 kms, &lt;em&gt;give or take&lt;/em&gt; (I know this is not geographically accurate):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/inegiR/imgs/spgg_grid.png&quot; alt=&quot;rough estimate of san pedro&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I feed the upper right hand and lower left hand coordinates to the function, and voila:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(inegiR)

upper_lat = 25.686917
upper_long = -100.429398
lower_lat = 25.612030
lower_long = -100.333032
token_denue &amp;lt;- &amp;quot;mytoken&amp;quot;

sanpedro &amp;lt;- denue_grid(upper_lat, lower_lat, 
                       upper_long, lower_long, 
                       token = token_denue)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Simple as that!&lt;/p&gt;

&lt;h2 id=&quot;factor-productivity&quot;&gt;Factor productivity&lt;/h2&gt;

&lt;p&gt;By using two fairly consistent surveys that INEGI makes on a monthly bases, I added two functions to calculate productivity, by state in two important industries.&lt;/p&gt;

&lt;p&gt;For both cases, productivity is defined as total value produced in state divided by number of total occupied people in the industry in the state. Bear in mind that value produced is in thousands of pesos, so 100 would be equal to 100 thousand pesos &lt;em&gt;“produced”&lt;/em&gt; by each person.&lt;/p&gt;

&lt;p&gt;We can simply get a time series by the doing the following:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(eem)
library(ggplot2)
# ts for Manufacturing in state of Nuevo León:
token &amp;lt;- &amp;quot;mytoken&amp;quot;
pm &amp;lt;- series_productividad_man(token)
nl &amp;lt;- data.frame(&amp;quot;Productivity&amp;quot; = pm$NL, &amp;quot;Date&amp;quot; = as.Date(pm$Fechas))
ggplot(nl, aes(x = Date, y = Productivity))+
  geom_line(colour = eem_colors[1])+
  theme_eem()+
  labs(title = &amp;quot;Productivity in Manufacturing \n State of Nuevo León&amp;quot;, 
        y = &amp;quot;Thousands of pesos x person&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/inegiR/imgs/manprod_nl.png&quot; alt=&quot;productividad manufacturera en nuevo león&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# ts for Construction in state of Nuevo León:
pc &amp;lt;- series_productividad_const(token)
nl &amp;lt;- data.frame(&amp;quot;Productivity&amp;quot; = pc$NL, &amp;quot;Date&amp;quot; = as.Date(pc$Fechas))
ggplot(nl, aes(x = Date, y = Productivity))+
  geom_line(colour = eem_colors[1])+
  theme_eem()+
  labs(title = &amp;quot;Productivity in Construction \n State of Nuevo León&amp;quot;, 
        y = &amp;quot;Thousands of pesos x person&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/inegiR/imgs/prodcons_nl.png&quot; alt=&quot;productividad construcción en nuevo león&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;new-geography&quot;&gt;New geography&lt;/h1&gt;

&lt;p&gt;These last two examples lead me to another point: the names in the functions with states have changed. In the first version, Nuevo León state was “NuevoLeon”, it has been changed to “NL”. This is more conscise, easier to read and consistent with the new constitutional name change for Mexico City (it is now “CDMX”, as opposed to “DF”).&lt;/p&gt;

&lt;p&gt;The other advantage is that these names will be consistent with Diego Valle’s &lt;code&gt;mxmaps&lt;/code&gt; package to easily make chroloplethr maps (it’s available &lt;a href=&quot;https://github.com/diegovalle/mxmaps&quot;&gt;here&lt;/a&gt;). There is a nifty function to make these included in the package using &lt;em&gt;inegiR&lt;/em&gt;, but now you can do this both ways!&lt;/p&gt;

&lt;p&gt;To switch between “old names” and the new ones, i’ve left the following catalog &lt;a href=&quot;https://github.com/Eflores89/proyectos/blob/master/data/inegiR_geo/df_oldnames.csv&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Name of State&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Previous Name&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;New Name&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Aguascalientes&lt;/td&gt;
      &lt;td&gt;Aguascalientes&lt;/td&gt;
      &lt;td&gt;AGS&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Baja California&lt;/td&gt;
      &lt;td&gt;BajaCalifornia&lt;/td&gt;
      &lt;td&gt;BC&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Baja California Sur&lt;/td&gt;
      &lt;td&gt;BajaCaliforniaSur&lt;/td&gt;
      &lt;td&gt;BCS&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Campeche&lt;/td&gt;
      &lt;td&gt;Campeche&lt;/td&gt;
      &lt;td&gt;CAMP&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Coahuila&lt;/td&gt;
      &lt;td&gt;Coahuila&lt;/td&gt;
      &lt;td&gt;COAH&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Colima&lt;/td&gt;
      &lt;td&gt;Colima&lt;/td&gt;
      &lt;td&gt;COL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Chiapas&lt;/td&gt;
      &lt;td&gt;Chiapas&lt;/td&gt;
      &lt;td&gt;CHPS&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Chihuahua&lt;/td&gt;
      &lt;td&gt;Chihuahua&lt;/td&gt;
      &lt;td&gt;CHIH&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Distrito Federal&lt;/td&gt;
      &lt;td&gt;DF&lt;/td&gt;
      &lt;td&gt;CDMX&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Durango&lt;/td&gt;
      &lt;td&gt;Durango&lt;/td&gt;
      &lt;td&gt;DGO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Guanajuato&lt;/td&gt;
      &lt;td&gt;Guanajuato&lt;/td&gt;
      &lt;td&gt;GTO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Guerrero&lt;/td&gt;
      &lt;td&gt;Guerrero&lt;/td&gt;
      &lt;td&gt;GRO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Hidalgo&lt;/td&gt;
      &lt;td&gt;Hidalgo&lt;/td&gt;
      &lt;td&gt;HGO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Jalisco&lt;/td&gt;
      &lt;td&gt;Jalisco&lt;/td&gt;
      &lt;td&gt;JAL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Estado de México&lt;/td&gt;
      &lt;td&gt;EdoMexico&lt;/td&gt;
      &lt;td&gt;MEX&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Michoacán&lt;/td&gt;
      &lt;td&gt;Michoacan&lt;/td&gt;
      &lt;td&gt;MICH&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Morelos&lt;/td&gt;
      &lt;td&gt;Morelos&lt;/td&gt;
      &lt;td&gt;MOR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nayarit&lt;/td&gt;
      &lt;td&gt;Nayarit&lt;/td&gt;
      &lt;td&gt;NAY&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nuevo León&lt;/td&gt;
      &lt;td&gt;NuevoLeon&lt;/td&gt;
      &lt;td&gt;NL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Oaxaca&lt;/td&gt;
      &lt;td&gt;Oaxaca&lt;/td&gt;
      &lt;td&gt;OAX&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Puebla&lt;/td&gt;
      &lt;td&gt;Puebla&lt;/td&gt;
      &lt;td&gt;PUE&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Querétaro&lt;/td&gt;
      &lt;td&gt;Queretaro&lt;/td&gt;
      &lt;td&gt;QRO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quintana Roo&lt;/td&gt;
      &lt;td&gt;QuintanaRoo&lt;/td&gt;
      &lt;td&gt;QROO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;San Luís Potosí&lt;/td&gt;
      &lt;td&gt;SanLuisPotosi&lt;/td&gt;
      &lt;td&gt;SLP&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sinaloa&lt;/td&gt;
      &lt;td&gt;Sinaloa&lt;/td&gt;
      &lt;td&gt;SIN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sonora&lt;/td&gt;
      &lt;td&gt;Sonora&lt;/td&gt;
      &lt;td&gt;SON&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Tabasco&lt;/td&gt;
      &lt;td&gt;Tabasco&lt;/td&gt;
      &lt;td&gt;TAB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Tamaulipas&lt;/td&gt;
      &lt;td&gt;Tamaulipas&lt;/td&gt;
      &lt;td&gt;TAM&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Tlaxcala&lt;/td&gt;
      &lt;td&gt;Tlaxcala&lt;/td&gt;
      &lt;td&gt;TLAX&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Veracruz&lt;/td&gt;
      &lt;td&gt;Veracruz&lt;/td&gt;
      &lt;td&gt;VER&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yucatán&lt;/td&gt;
      &lt;td&gt;Yucatan&lt;/td&gt;
      &lt;td&gt;YUC&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Zacatecas&lt;/td&gt;
      &lt;td&gt;Zacatecas&lt;/td&gt;
      &lt;td&gt;ZAC&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If there are any suggestions or bugs, you can find me at &lt;a href=&quot;https://twitter.com/eflores89&quot;&gt;twitter&lt;/a&gt; or &lt;a href=&quot;https://github.com/Eflores89&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sun, 21 Feb 2016 00:00:00 +0100</pubDate>
				<link>http://localhost:4000/datascience/inegir-v1-2/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/inegir-v1-2/</guid>
			</item>
		
			<item>
				<title>South Carolina Republican Debate with R</title>
					<description>&lt;p&gt;Continuing with the series analyzing republican debates, the latest in South Carolina confirms a few of the trends i’ve been observing, mainly that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Trump likes to repeat himself (maybe he likes to stick with what has worked so far?).&lt;/li&gt;
  &lt;li&gt;Trump also likes to keep it short: everyone else says more things for longer at a time.&lt;/li&gt;
  &lt;li&gt;The most relevant person in republican debates is actually a democrat: Mr. Obama. Only “people” was mentioned more often in the South Carolina.&lt;/li&gt;
  &lt;li&gt;Cruz vs. Trump is happening.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For those &lt;a href=&quot;http://enelmargen.org/datascience/r-debate-analysis-part-ii/&quot;&gt;following along&lt;/a&gt;, this is the new url ending:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;south_carolina &amp;lt;- &amp;quot;111395&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To save some precious real estate, i’ve done the work previously explained in the last posts, and built the data.frame with the debate. (You can see the &lt;em&gt;rough&lt;/em&gt; scripts &lt;a href=&quot;https://github.com/Eflores89/proyectos/tree/master/scripts&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;some-word-clouds&quot;&gt;Some word-clouds&lt;/h2&gt;

&lt;p&gt;I’ll start with the usual word-clouds. In this case, it’s actually interesting to see how the “war” between Cruz and Trump has been playing out.&lt;/p&gt;

&lt;p&gt;First we see an overall word-cloud (all the debates) for Mr. Cruz:
&lt;img src=&quot;/images/posts/cruz_wc_total.png&quot; alt=&quot;Ted Cruz all debates wordcloud&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But this last debate, the Donald made it to Ted’s most frequent words:
&lt;img src=&quot;/images/posts/cruz_wc_scarolina.png&quot; alt=&quot;Ted Cruz all debates wordcloud&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Trump, on the other hand, barely flinched. China was on his top agenda:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_sc_wc.png&quot; alt=&quot;Donald Trump south carolina debate word cloud&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;debate-cloud&quot;&gt;Debate-cloud&lt;/h2&gt;

&lt;p&gt;As for the entire debate, the most frequent words were “people” (intuitively makes sense) and “president”.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# all debates
scarolina &amp;lt;- apply(subset(all_debates, debate == &amp;quot;SouthCarolina&amp;quot;)[&amp;#39;message&amp;#39;],
                    1,
                    paste)
debate_cloud &amp;lt;- rquery.wordcloud(scarolina, 
    &amp;quot;text&amp;quot;, 
    max.words = 300,
    excludeWords = c(&amp;quot;going&amp;quot;,&amp;quot;and&amp;quot;,
                    &amp;quot;applause&amp;quot;,&amp;quot;get&amp;quot;,
                    &amp;quot;got&amp;quot;,&amp;quot;let&amp;quot;))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/all_scarolina_wc.png&quot; alt=&quot;Republican debate in South Carolina word cloud&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;individual-obsessions&quot;&gt;Individual obsessions&lt;/h2&gt;

&lt;p&gt;It’s also pretty clear from a few minutes of watching the debate, that candidates have their own quirky obsessions and mostly resort to talking about them as much as they can.&lt;/p&gt;

&lt;p&gt;First, a small function to give me the counts of words…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;TopMentions &amp;lt;- function(x){
df &amp;lt;- all_debates
  counts &amp;lt;- stri_count_regex(df$message, 
                            pattern = x,
                            case_insensitive = TRUE)
  df$counts &amp;lt;- counts
  df &amp;lt;- df %&amp;gt;% dplyr::group_by(person) %&amp;gt;%
                 dplyr::summarise(&amp;quot;mentions&amp;quot; = sum(counts))
  df &amp;lt;- as.data.frame(df)
  df &amp;lt;- subset(df, mentions&amp;gt;0)
  return(df)
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s see who likes to talk the most about guns…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# guns 
w &amp;lt;- c(&amp;quot;gun&amp;quot;,&amp;quot;guns&amp;quot;)
ggplot(order_axis(data = TopMentions(w),
                  axis = person, 
                  column = mentions), 
        aes(x = person_o, 
            y = mentions)) + 
        geom_bar(stat = &amp;quot;identity&amp;quot;, 
                 fill = eem_colors[1]) +
        theme_eem() +
        labs(x = &amp;quot;Person&amp;quot;, y = &amp;quot;Mentions&amp;quot;, 
        title = paste0(&amp;quot;Top mentions of &amp;quot;,w))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/topmention_guns.png&quot; alt=&quot;Top contenders mentioning guns&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I think we know who is gonna win here…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# mexico
w &amp;lt;- &amp;quot;mexico&amp;quot;
ggplot(order_axis(data = TopMentions(w),
                  axis = person, 
                  column = mentions), 
        aes(x = person_o, 
            y = mentions)) + 
        geom_bar(stat = &amp;quot;identity&amp;quot;, 
                 fill = eem_colors[1]) +
        theme_eem() +
        labs(x = &amp;quot;Person&amp;quot;, y = &amp;quot;Mentions&amp;quot;, 
        title = paste0(&amp;quot;Top mentions of &amp;quot;,w))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/topmention_mexico.png&quot; alt=&quot;Top contenders mentioning mexico&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Republicans are also characteristically strong on military-speak, so let’s see who likes this subject the most…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# isis, military
w &amp;lt;- c(&amp;quot;isis&amp;quot;,&amp;quot;military&amp;quot;)
ggplot(order_axis(data = TopMentions(w),
                  axis = person, 
                  column = mentions), 
        aes(x = person_o, 
            y = mentions)) + 
        geom_bar(stat = &amp;quot;identity&amp;quot;, 
                 fill = eem_colors[1]) +
        theme_eem() +
        labs(x = &amp;quot;Person&amp;quot;, y = &amp;quot;Mentions&amp;quot;, 
        title = paste0(&amp;quot;Top mentions of &amp;quot;,w))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/topmention_isis.png&quot; alt=&quot;Top contenders mentioning isis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the economic front, a natural contender pops up…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# taxes
w &amp;lt;- c(&amp;quot;taxes&amp;quot;)
ggplot(order_axis(data = TopMentions(w),
                  axis = person, 
                  column = mentions), 
        aes(x = person_o, 
            y = mentions)) + 
        geom_bar(stat = &amp;quot;identity&amp;quot;, 
                 fill = eem_colors[1]) +
        theme_eem() +
        labs(x = &amp;quot;Person&amp;quot;, y = &amp;quot;Mentions&amp;quot;, 
        title = paste0(&amp;quot;Top mentions of &amp;quot;,w))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/topmention_taxes.png&quot; alt=&quot;Top contenders mentioning taxes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And finally… this a surprise!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# muslim
w &amp;lt;- &amp;quot;muslim&amp;quot;
ggplot(order_axis(data = TopMentions(w),
                  axis = person, 
                  column = mentions), 
        aes(x = person_o, 
            y = mentions)) + 
        geom_bar(stat = &amp;quot;identity&amp;quot;, 
                 fill = eem_colors[1]) +
        theme_eem() +
        labs(x = &amp;quot;Person&amp;quot;, y = &amp;quot;Mentions&amp;quot;, 
        title = paste0(&amp;quot;Top mentions of &amp;quot;,w))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/topmention_muslim.png&quot; alt=&quot;Top contenders mentioning muslim&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I guess it’s worth mentioning a caveat here: this has nothing to do with sentiment, Mr. Bush is arguably much nicer in tone that Trump…&lt;/p&gt;

&lt;p&gt;Also, some candidates prefer another term…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;w &amp;lt;- &amp;quot;islamic&amp;quot;
ggplot(order_axis(data = TopMentions(w),
                  axis = person, 
                  column = mentions), 
        aes(x = person_o, 
            y = mentions)) + 
        geom_bar(stat = &amp;quot;identity&amp;quot;, 
                 fill = eem_colors[1]) +
        theme_eem() +
        labs(x = &amp;quot;Person&amp;quot;, y = &amp;quot;Mentions&amp;quot;, 
        title = paste0(&amp;quot;Top mentions of &amp;quot;,w))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/topmentions_islamic.png&quot; alt=&quot;Top contenders mentioning islamic&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;aggregate-stats&quot;&gt;Aggregate stats&lt;/h2&gt;

&lt;p&gt;Let’s see what the aggregate stats have to say about this new debate…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;candidates &amp;lt;- c(&amp;quot;TRUMP&amp;quot;,&amp;quot;CARSON&amp;quot;,&amp;quot;RUBIO&amp;quot;,
                &amp;quot;KASICH&amp;quot;,&amp;quot;CRUZ&amp;quot;,&amp;quot;BUSH&amp;quot;,
                &amp;quot;FIORINA&amp;quot;,&amp;quot;PAUL&amp;quot;,&amp;quot;CHRISTIE&amp;quot;)
info &amp;lt;- NULL
info_all &amp;lt;- NULL
for(i in 1:9){
info &amp;lt;- UnlistAndExtractInfo(candidates[i])
info$CANDIDATE &amp;lt;- candidates[i]
info_all &amp;lt;- rbind(info_all, info)
}

# i&amp;#39;m going to add a few more columns...
info_all %&amp;lt;&amp;gt;% mutate(carry_over_p1 = unique_words_repeated_fromfirst/words_unique,
                     word_repeat = words_total/words_unique)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Using this information to graph…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot(subset(info_all, CANDIDATE != &amp;quot;CHRISTIE&amp;quot; &amp;amp; words_unique&amp;gt;90), 
       aes(x = words_total, 
           y = words_unique)) + 
    geom_point(aes(colour = CANDIDATE), size = 3, shape = 2) +
    stat_smooth()+
    theme_eem()+ # uses &amp;quot;eflores89/eem&amp;quot;
    scale_colour_eem(20) + # uses &amp;quot;eflores89/eem&amp;quot;
    labs(title = &amp;quot;Words per Debate&amp;quot;,
         x = &amp;quot;Total Words&amp;quot;, 
         y = &amp;quot;Unique Words&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/debate6_words.png&quot; alt=&quot;unique words vs words by debate&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m not an expert, but I would argue that the longer the debate format, the worst off Trump will likely do. Why?&lt;/p&gt;

&lt;p&gt;There seems to be a declining curve once about 3,000 words are spoken. Whereas Ted Cruz can fluently speak about more things (he says more unique words), Trump seems to be struggling to find new words.&lt;/p&gt;

&lt;p&gt;Of course, I’m assuming saying the same thing over and over again is bad for your campaign. Perhaps the formula is the other way around.&lt;/p&gt;

&lt;p&gt;But again, let’s see this trend more clearly with another graph…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# average times unique word is repeated...
ggplot(subset(info_all, CANDIDATE != &amp;quot;CHRISTIE&amp;quot; &amp;amp; words_unique&amp;gt;90), 
       aes(x = factor(CANDIDATE), 
           y = word_repeat, 
           fill = eem_colors[1])) +
  geom_boxplot() +
  theme_eem()+
  labs(title = &amp;quot;Average repetition of unique words&amp;quot;,
       x = &amp;quot;Candidate&amp;quot;, 
       y = &amp;quot;Repetitions&amp;quot;) + theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_reps6.png&quot; alt=&quot;words per unique word&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The trend continues. Trump says considerably more times each unique word than the other candidates.&lt;/p&gt;

&lt;h2 id=&quot;speed-of-intervention&quot;&gt;Speed of Intervention&lt;/h2&gt;

&lt;p&gt;Like the debate in Las Vegas, South Carolina was similar for Trump in terms of words spoken in each “intervention” (time he was continually speaking). As I said earlier, this can simply be due to the fact that he has to constantly play defense…&lt;/p&gt;

&lt;p&gt;Kasich, on the other hand, delivers his remarks and follows the rules: he does not speak when not spoken to (which is most of the debate).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# order the debates...
info_all$debate &amp;lt;- factor(info_all$debate, 
                          levels = c(&amp;quot;Ohio&amp;quot;,&amp;quot;California&amp;quot;,
                                     &amp;quot;Colorado&amp;quot;,&amp;quot;Wisconsin&amp;quot;,
                                     &amp;quot;Vegas&amp;quot;, &amp;quot;SCarolina&amp;quot;))

# average length of interventions
ggplot(info_all, 
       aes(x = debate, 
           y = average_intervention, 
           group = CANDIDATE)) + 
  geom_path(aes(colour = CANDIDATE)) + 
  theme_eem() + 
  scale_colour_eem(20) + 
  labs(x = &amp;quot;Debate&amp;quot;, 
       y = &amp;quot;Words&amp;quot;, 
       title = &amp;quot;Average words per intervention&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/words_trend6.png&quot; alt=&quot;words by intervention&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-data&quot;&gt;The Data&lt;/h2&gt;
&lt;p&gt;As usual, I’ve left this data openly available via github, so anyone can play around with it and find a few more insights. &lt;a href=&quot;https://github.com/Eflores89/proyectos/tree/master/data/us_debates&quot;&gt;Here&lt;/a&gt; is the link.&lt;/p&gt;

&lt;p&gt;You can also contact me via twitter for any questions: @eflores89 or via an issue in github.&lt;/p&gt;
</description>
				<pubDate>Tue, 19 Jan 2016 00:00:00 +0100</pubDate>
				<link>http://localhost:4000/datascience/r-debate-analysis-part-iii/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/r-debate-analysis-part-iii/</guid>
			</item>
		
			<item>
				<title>More republican debate analysis with R</title>
					<description>&lt;p&gt;A few weeks late, here is a follow-up analysis using &lt;em&gt;R&lt;/em&gt;, of the transcript of the latest Republican primary debate held at Las Vegas, Nevada.&lt;/p&gt;

&lt;p&gt;Like the previous post, it should be interesting to see some word-clouds and some trends from the front-runners (and &lt;em&gt;of course&lt;/em&gt;, Donald Trump).&lt;/p&gt;

&lt;h2 id=&quot;getting-and-cleaning-the-data&quot;&gt;Getting and cleaning the data&lt;/h2&gt;

&lt;p&gt;As in the last post, we’re going to import the data and clean with a function that was &lt;a href=&quot;http://enelmargen.org/datascience/republican-debates/index.html&quot;&gt;nicely improved by Alan Jordan&lt;/a&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# some packages for scraping and cleaning the data
library(rvest)
library(plyr)
library(dplyr)
library(stringi)
library(magrittr)

# function to partially separate and clean into a data.frame a debate from the presidency project
MakeDebateDF&amp;lt;-function(df){
  newdf &amp;lt;- data.frame(
    person = apply(df, 
                   MARGIN = 1, 
                   function(x){
                     stri_extract_first_regex(x, 
                                              &amp;quot;[A-Z&amp;#39;-]+(?=(:\\s))&amp;quot;)
                   }),
    message = apply(df, 
                    MARGIN = 1, 
                    function(x){
                      stri_replace_first_regex(x,
                                               &amp;quot;[A-Z&amp;#39;-]+:\\s+&amp;quot;, 
                                               &amp;quot;&amp;quot;)
                    }),
    stringsAsFactors=FALSE
  )
  for (j in 2:nrow(newdf)) { 
  if (is.na(newdf[j,&amp;#39;person&amp;#39;])) 
		{newdf[j,&amp;#39;person&amp;#39;] &amp;lt;-  newdf[(j-1),&amp;#39;person&amp;#39;] }
	}

  return(newdf)
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This time i’m only downloading one debate, and joining with the last four I had parsed…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Importing debates --- 
# url for all debates
url &amp;lt;- &amp;quot;http://www.presidency.ucsb.edu/ws/index.php?pid=&amp;quot;

### -------- debate in Las Vegas, Nevada (fifth debate)
lasvegas &amp;lt;- &amp;quot;111177&amp;quot;

debate_v &amp;lt;- read_html(paste0(url, lasvegas)) %&amp;gt;% 
  html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
  html_text()

debate_v &amp;lt;- ldply(debate_v, rbind)
debate_v &amp;lt;- MakeDebateDF(debate_v)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;analyzing&quot;&gt;Analyzing&lt;/h2&gt;
&lt;p&gt;Let’s join this data with the previous debates and see some stats and wordclouds…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# the last 4 debates were stored in &amp;quot;all_debates&amp;quot; object...
all_debates &amp;lt;- rbind(all_debates, 
                     debate_v)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Because he’s the most interesting to watch, let’s see what Trump says overall and in this debate…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(ggplot2)
# this is for order_axis and theme_eem
# it can be downloaded using 
# devtools::install_github(&amp;quot;eflores89/eem&amp;quot;)
library(eem)
# all debates
trump_words &amp;lt;- apply(subset(all_debates, person == &amp;quot;TRUMP&amp;quot;)[&amp;#39;message&amp;#39;],
                    1,
                    paste)
# cloud
# function taken from: 
# http://www.sthda.com/english/wiki/word-cloud-generator-in-r-one-killer-function-to-do-everything-you-need
trump_cloud &amp;lt;- rquery.wordcloud(trump_words, 
    &amp;quot;text&amp;quot;, 
    max.words = 300,
    excludeWords = c(&amp;quot;going&amp;quot;,&amp;quot;and&amp;quot;,
                    &amp;quot;applause&amp;quot;,&amp;quot;get&amp;quot;,
                    &amp;quot;got&amp;quot;,&amp;quot;let&amp;quot;))

trump_freq &amp;lt;- trump_cloud$freqTable

# debate in Las Vegas
trump_words_l &amp;lt;- apply(subset(debate_v, person == &amp;quot;TRUMP&amp;quot;)[&amp;#39;message&amp;#39;],
                    1,
                    paste)
trump_cloud_l &amp;lt;- rquery.wordcloud(trump_words_l, 
    &amp;quot;text&amp;quot;, 
    max.words = 300,
    excludeWords = c(&amp;quot;going&amp;quot;,&amp;quot;and&amp;quot;,
                    &amp;quot;applause&amp;quot;,&amp;quot;get&amp;quot;,
                    &amp;quot;got&amp;quot;,&amp;quot;let&amp;quot;))

trump_freq_l &amp;lt;- trump_cloud_l$freqTable&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;overall-word-cloud&quot;&gt;Overall word-cloud&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_cloud2.png&quot; alt=&quot;Donald Trump all debates wordcloud&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;las-vegas&quot;&gt;Las Vegas&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_cloud_lv.png&quot; alt=&quot;Donald Trump las vegas debate word cloud&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;shifts-in-speech&quot;&gt;Shifts in speech&lt;/h2&gt;

&lt;p&gt;Of course, over the same five debates, topics have shifted tremendously both among the contenders and Trump.&lt;/p&gt;

&lt;p&gt;For example, let’s see what the most spoken words were by debate…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# using previous data for each debate....
debate_words_h &amp;lt;- rquery.wordcloud(x = debate_h$message) #ohio, 1st
  # just the frequency table...
  # a bit lazy to do myself!
  debate_words_h &amp;lt;- debate_words_h$freq %&amp;gt;% mutate(&amp;quot;Debate&amp;quot; = &amp;quot;Ohio&amp;quot;)
debate_words_c &amp;lt;- rquery.wordcloud(x = debate_c$message) #cali, 2nd
  debate_words_c &amp;lt;- debate_words_c$freq %&amp;gt;% mutate(&amp;quot;Debate&amp;quot; = &amp;quot;California&amp;quot;)
debate_words_b &amp;lt;- rquery.wordcloud(x = debate_b$message) #boulder, 3rd
  debate_words_b &amp;lt;- debate_words_b$freq %&amp;gt;% mutate(&amp;quot;Debate&amp;quot; = &amp;quot;Boulder&amp;quot;)
debate_words_w &amp;lt;- rquery.wordcloud(x = debate_w$message) #wisc, 4th
  debate_words_w &amp;lt;- debate_words_w$freq %&amp;gt;% mutate(&amp;quot;Debate&amp;quot; = &amp;quot;Wisconsin&amp;quot;)
debate_words_v &amp;lt;- rquery.wordcloud(x = debate_v$message) #vegas, 5th
  debate_words_v &amp;lt;- debate_words_v$freq %&amp;gt;% mutate(&amp;quot;Debate&amp;quot; = &amp;quot;LasVegas&amp;quot;)

# join all
all_debate_words &amp;lt;- rbind.data.frame(debate_words_h, debate_words_c) 
all_debate_words &amp;lt;- rbind.data.frame(all_debate_words, debate_words_b) 
all_debate_words &amp;lt;- rbind.data.frame(all_debate_words, debate_words_w) 
all_debate_words &amp;lt;- rbind.data.frame(all_debate_words, debate_words_v) 

# graph with some interesting words...
interesting_words &amp;lt;- subset(all_debate_words, word %in% c(&amp;quot;government&amp;quot;,
                                              &amp;quot;isis&amp;quot;,&amp;quot;president&amp;quot;,&amp;quot;senator&amp;quot;,
                                              &amp;quot;money&amp;quot;, &amp;quot;jobs&amp;quot;, &amp;quot;tax&amp;quot;, &amp;quot;obama&amp;quot;,
                                              &amp;quot;clinton&amp;quot;, &amp;quot;america&amp;quot;))

interesting_words$Debate &amp;lt;- factor(interesting_words$Debate, 
                          levels = c(&amp;quot;Ohio&amp;quot;,&amp;quot;California&amp;quot;,
                                     &amp;quot;Boulder&amp;quot;,&amp;quot;Wisconsin&amp;quot;,
                                     &amp;quot;LasVegas&amp;quot;))

ggplot(data = interesting_words, 
        aes(x = Debate, 
            y = freq, 
            group = word)) + 
        geom_line(aes(colour = word)) +
        theme_eem() +
        scale_colour_eem(20) + 
        labs(x = &amp;quot;Debate&amp;quot;, 
             y = &amp;quot;Frequency&amp;quot;, 
             title = &amp;quot;Shifts in speech&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Apparently, “tax” is out: it wasn’t even mentioned this past debate, in contrast with the increasingly present “isis”. “Clinton” and “obama” are a constant:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/shifts_p2.png&quot; alt=&quot;shifts in speech republican debates&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;aggregate-stats&quot;&gt;Aggregate stats&lt;/h2&gt;

&lt;p&gt;Now lets see some aggregate stats by contender.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This function is a bit confusing and/or unnecesary, I’ll probably find a better way to do this in the future…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;UnlistAndExtractInfo &amp;lt;- function(candidate){
# this function is not general - it only applies to these particular debates...
# all the debates must be named the same in the parent env.
# for example: debate_h ...

allwords_1 &amp;lt;- tolower(unlist(
              stri_extract_all_words(
              apply(
              subset(debate_h, person == candidate)[&amp;#39;message&amp;#39;],
                    1,
                    paste))))
allwords_2 &amp;lt;- tolower(unlist(
              stri_extract_all_words(
              apply(
              subset(debate_c, person == candidate)[&amp;#39;message&amp;#39;],
                    1,
                    paste))))
allwords_3 &amp;lt;- tolower(unlist(
              stri_extract_all_words(
              apply(
              subset(debate_b, person == candidate)[&amp;#39;message&amp;#39;],
                    1,
                    paste))))
allwords_4 &amp;lt;- tolower(unlist(
              stri_extract_all_words(
              apply(
              subset(debate_w, person == candidate)[&amp;#39;message&amp;#39;],
                    1,
                    paste))))
allwords_5 &amp;lt;- tolower(unlist(
              stri_extract_all_words(
              apply(
              subset(debate_v, person == candidate)[&amp;#39;message&amp;#39;],
                    1,
                    paste))))
df_insights &amp;lt;- data.frame(
debate = c(&amp;quot;Ohio&amp;quot;, &amp;quot;California&amp;quot;, &amp;quot;Colorado&amp;quot;, &amp;quot;Wisconsin&amp;quot;,&amp;quot;Vegas&amp;quot;),
average_intervention = c(mean(stri_count_words(
                        apply(
                          subset(debate_h, person == candidate)[&amp;#39;message&amp;#39;],
                                  1,
                        paste))),
                        mean(stri_count_words(
                        apply(
                          subset(debate_c, person == candidate)[&amp;#39;message&amp;#39;],
                                  1,
                        paste))),
                        mean(stri_count_words(
                        apply(
                          subset(debate_b, person == candidate)[&amp;#39;message&amp;#39;],
                                  1,
                        paste))),
                        mean(stri_count_words(
                        apply(
                          subset(debate_w, person == candidate)[&amp;#39;message&amp;#39;],
                                  1,
                        paste))),
                        mean(stri_count_words(
                        apply(
                          subset(debate_c, person == candidate)[&amp;#39;message&amp;#39;],
                                  1,
                        paste)))
                        ),
words_total = c(length(allwords_1),
                length(allwords_2),
                length(allwords_3),
                length(allwords_4),
                length(allwords_5)),
words_unique = c(length(unique(allwords_1)),
                 length(unique(allwords_2)),
                 length(unique(allwords_3)),
                 length(unique(allwords_4)),
                 length(unique(allwords_5))
                 ),
words_repeated_fromfirst = c(0, sum(allwords_2 %in% allwords_1), 
                            sum(allwords_3 %in% allwords_1),
                            sum(allwords_4 %in% allwords_1),
                            sum(allwords_5 %in% allwords_1)),
unique_words_repeated_fromfirst = c(0,
                            length(unique(allwords_2[allwords_2 %in% allwords_1])),
                            length(unique(allwords_3[allwords_3 %in% allwords_1])),
                            length(unique(allwords_4[allwords_4 %in% allwords_1])),
                            length(unique(allwords_5[allwords_5 %in% allwords_1]))
                            ),
words_repeated_fromsecond = c(0, 0, 
                            sum(allwords_3 %in% allwords_2),
                            sum(allwords_4 %in% allwords_2),
                            sum(allwords_5 %in% allwords_2)),
unique_words_repeated_fromsecond = c(0, 0,
                            length(unique(allwords_3[allwords_3 %in% allwords_2])),
                            length(unique(allwords_4[allwords_4 %in% allwords_2])),
                            length(unique(allwords_5[allwords_5 %in% allwords_2]))
                            ),
words_repeated_fromthird = c(0, 0, 0,
                            sum(allwords_4 %in% allwords_3),
                            sum(allwords_5 %in% allwords_3)),
unique_words_repeated_fromthird = c(0, 0, 0,
                            length(unique(allwords_4[allwords_4 %in% allwords_3])),
                            length(unique(allwords_5[allwords_5 %in% allwords_3]))
                            )
, stringsAsFactors = FALSE)
return(df_insights)
}

# going to create a data frame with all the counts from the top candidates...
candidates &amp;lt;- c(&amp;quot;TRUMP&amp;quot;,&amp;quot;CARSON&amp;quot;,&amp;quot;RUBIO&amp;quot;,
                &amp;quot;KASICH&amp;quot;,&amp;quot;CRUZ&amp;quot;,&amp;quot;BUSH&amp;quot;,
                &amp;quot;FIORINA&amp;quot;,&amp;quot;PAUL&amp;quot;,&amp;quot;CHRISTIE&amp;quot;)
info &amp;lt;- NULL
info_all &amp;lt;- NULL
for(i in 1:9){
info &amp;lt;- UnlistAndExtractInfo(candidates[i])
info$CANDIDATE &amp;lt;- candidates[i]
info_all &amp;lt;- rbind(info_all, info)
}

# i&amp;#39;m going to add a few more columns...
info_all %&amp;lt;&amp;gt;% mutate(carry_over_p1 = unique_words_repeated_fromfirst/words_unique,
                     word_repeat = words_total/words_unique)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Using this information to graph…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# graph of most words spoken by debate
ggplot(order_axis(
  subset(info_all, debate != &amp;quot;Ohio&amp;quot; &amp;amp; CANDIDATE != &amp;quot;CHRISTIE&amp;quot;), # christie didn&amp;#39;t go to wisconsin
    CANDIDATE, carry_over_p1), 
       aes(x = CANDIDATE_o, 
           y = carry_over_p1)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;, 
           aes(fill = CANDIDATE_o)) + 
  facet_grid(debate ~.) + 
  theme_eem() +
  scale_fill_eem(20) + 
  labs(title = &amp;quot;Repetition of words by candidate&amp;quot;, 
       x = &amp;quot;Candidate&amp;quot;, 
       y = &amp;quot;% of unique words repeated from first debate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As the graph shows, Trump continues to lead in repetitiveness. In the latest debate, the Donald repeated 44.8% of the words he said during the first debate, followed by 38% from Kasich and 36% from Bush.&lt;/p&gt;

&lt;p&gt;This is a key metric Trump has been consistently winning…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/reps_part2.png&quot; alt=&quot;repetitions_trump&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, if we plot total words versus unique words, to find the repetition of each individual word, we find Mr. Trump consistently below the trend: he says each word much more than the average candidate.&lt;/p&gt;

&lt;p&gt;On the other hand, Carson and Fiorina tend to have a larger vocabulary of words.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot(subset(info_all,CANDIDATE != &amp;quot;CHRISTIE&amp;quot;), 
       aes(x = words_total, 
           y = words_unique)) + 
    geom_point(aes(colour = CANDIDATE), size = 3, shape = 2) +
    stat_smooth()+
    theme_eem()+ # uses &amp;quot;eflores/eem&amp;quot;
    scale_colour_eem(20) + # uses &amp;quot;eflores/eem&amp;quot;
    labs(title = &amp;quot;Words per Debate&amp;quot;,
         x = &amp;quot;Total Words&amp;quot;, 
         y = &amp;quot;Unique Words&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/words_debate_2.png&quot; alt=&quot;unique words vs words by debate&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aggregating over the whole gives us a sense of this difference much more clearly:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# average times unique word is repeated...

ggplot(info_all, 
       aes(x = factor(CANDIDATE), 
           y = word_repeat, fill = eem_colors[1])) +
  geom_boxplot() +
  theme_eem()+
  labs(title = &amp;quot;Average repetition of unique words&amp;quot;,
       x = &amp;quot;Candidate&amp;quot;, 
       y = &amp;quot;Repetitions&amp;quot;) + theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/wordsxunique_2.png&quot; alt=&quot;words per unique word&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;speed-of-intervention&quot;&gt;Speed of Intervention&lt;/h2&gt;

&lt;p&gt;This last debate also had the effect of spreading the gap between Trump and his opponents in terms of speed in interventions. Every time he talks, he always says less words, but this was even more apparent in Las Vegas…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# order the debates...
info_all$debate &amp;lt;- factor(info_all$debate, 
                          levels = c(&amp;quot;Ohio&amp;quot;,&amp;quot;California&amp;quot;,
                                     &amp;quot;Colorado&amp;quot;,&amp;quot;Wisconsin&amp;quot;,
                                     &amp;quot;Vegas&amp;quot;))

# average length of interventions
ggplot(info_all, 
       aes(x = debate, 
           y = average_intervention, 
           group = CANDIDATE)) + 
  geom_path(aes(colour = CANDIDATE)) + 
  theme_eem() + 
  scale_colour_eem(20) + 
  labs(x = &amp;quot;Debate&amp;quot;, 
       y = &amp;quot;Words&amp;quot;, 
       title = &amp;quot;Average words per intervention&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/intervention_debs.png&quot; alt=&quot;words by intervention&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This can also be an indication of how popular he is or how much “hits” he’s taking. When you need to counter an argument, sometimes only a few words is enough. If you do this constantly more than the others, the average is bound to go down.&lt;/p&gt;

&lt;h2 id=&quot;the-data&quot;&gt;The Data&lt;/h2&gt;

&lt;p&gt;As Alan Jordan suggested, i’ve left this data openly available via github, so anyone can play around with it and find a few more insights. &lt;a href=&quot;https://github.com/Eflores89/proyectos/tree/master/data/us_debates&quot;&gt;Here&lt;/a&gt; is the link.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;code&gt;all_debates&lt;/code&gt; data.frame contains two columns: candidate and message. This is all of the debates.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;debate_h&lt;/code&gt; is the Ohio debate.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;debate_c&lt;/code&gt; is the California debate.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;debate_b&lt;/code&gt; is the Boulder debate.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;debate_w&lt;/code&gt; is the Wisconsin debate.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;debate_v&lt;/code&gt; is the Las Vegas debate.&lt;/li&gt;
  &lt;li&gt;The &lt;code&gt;info_all&lt;/code&gt; data.frame is the aggregate stats of contenders by debate. It contains word counts, unique word counts, etc.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Thu, 07 Jan 2016 00:00:00 +0100</pubDate>
				<link>http://localhost:4000/datascience/r-debate-analysis-part-ii/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/r-debate-analysis-part-ii/</guid>
			</item>
		
			<item>
				<title>What the candidates say, analyzing republican debates using R</title>
					<description>&lt;p&gt;As most people realize, this is probably one of the most data-rich primary campaigns in history, with hundreds of professional pollsters poring over every data-point trying to understand voter’s intention.&lt;/p&gt;

&lt;p&gt;So here is another data-rich post to that end.&lt;/p&gt;

&lt;p&gt;I was glad to discover the University of California at Santa Barbara’s &lt;a href=&quot;http://www.presidency.ucsb.edu/&quot;&gt;webpage&lt;/a&gt; with tons of high-quality data related to the elections.&lt;/p&gt;

&lt;p&gt;Amongst these are the transcripts of presidential debates going back to 1960, which I will pore over a bit further.&lt;/p&gt;

&lt;p&gt;Because the Republican race is arguably more fun to watch, i’ll be concentrating on these debates.&lt;/p&gt;

&lt;h2 id=&quot;getting-and-cleaning-the-data&quot;&gt;Getting and cleaning the data&lt;/h2&gt;

&lt;p&gt;A few things to set-up before downloading the data:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Update (1/4/2015) - Thanks to Alan Jordan who nicely corrected my regex with a more robust version and a more sucinct function for cleaning the data below..&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# some packages for scraping and cleaning the data
library(rvest)
library(plyr)
library(dplyr)
library(stringi)
library(magrittr)

# function to partially separate and clean into a data.frame a debate from the presidency project
MakeDebateDF&amp;lt;-function(df){
  newdf &amp;lt;- data.frame(
    person = apply(df, 
                   MARGIN = 1, 
                   function(x){
                     stri_extract_first_regex(x, 
                                              &amp;quot;[A-Z&amp;#39;-]+(?=(:\\s))&amp;quot;)
                   }),
    message = apply(df, 
                    MARGIN = 1, 
                    function(x){
                      stri_replace_first_regex(x,
                                               &amp;quot;[A-Z&amp;#39;-]+:\\s+&amp;quot;, 
                                               &amp;quot;&amp;quot;)
                    }),
    stringsAsFactors=FALSE
  )
  for (j in 2:nrow(newdf)) { 
  if (is.na(newdf[j,&amp;#39;person&amp;#39;])) 
		{newdf[j,&amp;#39;person&amp;#39;] &amp;lt;-  newdf[(j-1),&amp;#39;person&amp;#39;] }
	}

  return(newdf)
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now, to download the last 4 debates (i’m only going to analyze the “big-boy” debates between top contenders and omit New Hampshire because it is not in the database). I’ll use the main webpage for the presidency project:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Importing debates --- 
# url for all debates
url &amp;lt;- &amp;quot;http://www.presidency.ucsb.edu/ws/index.php?pid=&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And download and fix each debate:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;### -------- debate in Wisconsin (fourth debate)
wisconsin &amp;lt;- &amp;quot;110908&amp;quot;

debate_w &amp;lt;- read_html(paste0(url, wisconsin)) %&amp;gt;% 
  html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
  html_text()

debate_w &amp;lt;- ldply(debate_w, rbind)
debate_w &amp;lt;- MakeDebateDF(debate_w)

### -------- debate in Boulder, Col. (third debate)
boulder &amp;lt;- &amp;quot;110906&amp;quot;

debate_b &amp;lt;- read_html(paste0(url, boulder)) %&amp;gt;% 
  html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
  html_text()

debate_b &amp;lt;- ldply(debate_b, rbind)
debate_b &amp;lt;- MakeDebateDF(debate_b)

### -------- debate in Simi Valley, California (second debate)
california &amp;lt;- &amp;quot;110756&amp;quot;

debate_c &amp;lt;- read_html(paste0(url, california)) %&amp;gt;% 
  html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
  html_text()

debate_c &amp;lt;- ldply(debate_c, rbind)
debate_c &amp;lt;- MakeDebateDF(debate_c)

### -------- debate in Cleveland, Ohio (first debate)
ohio &amp;lt;- &amp;quot;110489&amp;quot;

debate_h &amp;lt;- read_html(paste0(url, ohio)) %&amp;gt;% 
  html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
  html_text()

debate_h &amp;lt;- ldply(debate_h, rbind)
debate_h &amp;lt;- MakeDebateDF(debate_h)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;analyzing&quot;&gt;Analyzing&lt;/h2&gt;

&lt;p&gt;Now, for the fun part. First, let’s start with some simply word-clouds (using &lt;a href=&quot;http://www.sthda.com/english/wiki/word-cloud-generator-in-r-one-killer-function-to-do-everything-you-need&quot;&gt;this excellent example&lt;/a&gt; as a starting point)&lt;/p&gt;

&lt;p&gt;I’m going to use the &lt;code&gt;rquery.wordcloud&lt;/code&gt; the function, taken shamelessly from sthda.com in the previous link to see what contenders like to talk about the most…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Join into large d.f.
all_debates &amp;lt;- rbind(debate_w, 
                     debate_b,
                     debate_c,
                     debate_h)

# these are necesary for plots 
library(ggplot2)
# this is for order_axis and theme_eem
# it can be downloaded using 
# devtools::install_github(&amp;quot;eflores/eem&amp;quot;)
library(eem)

trump_words &amp;lt;- apply(subset(all_debates, person == &amp;quot;TRUMP&amp;quot;)[&amp;#39;message&amp;#39;],
                    1,
                    paste)
# cloud
trump_cloud &amp;lt;- rquery.wordcloud(trump_words, 
    &amp;quot;text&amp;quot;, 
    max.words = 300,
    excludeWords = c(&amp;quot;going&amp;quot;,&amp;quot;and&amp;quot;,
                    &amp;quot;applause&amp;quot;,&amp;quot;get&amp;quot;,
                    &amp;quot;got&amp;quot;,&amp;quot;let&amp;quot;))

trump_freq &amp;lt;- trump_cloud$freqTable

# top 10
trump_top &amp;lt;- ggplot(order_axis(
      trump_freq[1:10,],
        word, freq), 
      aes(x = word_o, 
          y = freq))+
    geom_bar(stat=&amp;quot;identity&amp;quot;,
              fill = eem_colors[1]) +
    theme_eem() + 
    labs(title = &amp;quot;Top 10 words in Debates \n Donald Trump&amp;quot;, 
          x = &amp;quot;Word&amp;quot;,
          y = &amp;quot;Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Donald really likes “great” more than the other candidates…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_wordcloud.png&quot; alt=&quot;trump_wordcloud&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/trump_top10.png&quot; alt=&quot;trump_top10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using the same process… What about Jeb Bush? He prefers to mention Hillary…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/bush_wordcloud.png&quot; alt=&quot;bush_wordcloud&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/bush_top10.png&quot; alt=&quot;bush_top10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Marco Rubio likes to present his tax plan…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/rubio_wordcloud.png&quot; alt=&quot;rubio_wordcloud&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/rubio_top10.png&quot; alt=&quot;rubio_top10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the notoriously outspoken Cruz omits “people”, talks tax and prefers to confront “washington” more than his colleagues:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/cruz_wordcloud.png&quot; alt=&quot;cruz_wordcloud&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/cruz_top10.png&quot; alt=&quot;cruz_top10&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;more-stats&quot;&gt;More stats!&lt;/h2&gt;

&lt;p&gt;The former stats are all about the total participation in debates, but more interesting is probably the way these candidates have (if they have), shifted views over the course of these debates.&lt;/p&gt;

&lt;p&gt;It would be interesting to do some simple arithmetic on the corpus of words…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;UnlistAndExtractInfo &amp;lt;- function(candidate){
# this function is not general - it only applies to these particular debates...
# all the debates must be named the same in the parent env.
# for example: debate_h ...

allwords_1 &amp;lt;- tolower(unlist(
              stri_extract_all_words(
              apply(
              subset(debate_h, person == candidate)[&amp;#39;message&amp;#39;],
                    1,
                    paste))))
allwords_2 &amp;lt;- tolower(unlist(
              stri_extract_all_words(
              apply(
              subset(debate_c, person == candidate)[&amp;#39;message&amp;#39;],
                    1,
                    paste))))
allwords_3 &amp;lt;- tolower(unlist(
              stri_extract_all_words(
              apply(
              subset(debate_b, person == candidate)[&amp;#39;message&amp;#39;],
                    1,
                    paste))))
allwords_4 &amp;lt;- tolower(unlist(
              stri_extract_all_words(
              apply(
              subset(debate_w, person == candidate)[&amp;#39;message&amp;#39;],
                    1,
                    paste))))
df_insights &amp;lt;- data.frame(
debate = c(&amp;quot;Ohio&amp;quot;, &amp;quot;California&amp;quot;, &amp;quot;Colorado&amp;quot;, &amp;quot;Wisconsin&amp;quot;),
average_intervention = c(mean(stri_count_words(
                        apply(
                          subset(debate_h, person == candidate)[&amp;#39;message&amp;#39;],
                                  1,
                        paste))),
                        mean(stri_count_words(
                        apply(
                          subset(debate_c, person == candidate)[&amp;#39;message&amp;#39;],
                                  1,
                        paste))),
                        mean(stri_count_words(
                        apply(
                          subset(debate_b, person == candidate)[&amp;#39;message&amp;#39;],
                                  1,
                        paste))),
                        mean(stri_count_words(
                        apply(
                          subset(debate_w, person == candidate)[&amp;#39;message&amp;#39;],
                                  1,
                        paste)))
                        ),
words_total = c(length(allwords_1),length(allwords_2),length(allwords_3),length(allwords_4)),
words_unique = c(length(unique(allwords_1)),
                 length(unique(allwords_2)),
                 length(unique(allwords_3)),
                 length(unique(allwords_4))),
words_repeated_fromfirst = c(0, sum(allwords_2 %in% allwords_1), 
                            sum(allwords_3 %in% allwords_1),
                            sum(allwords_4 %in% allwords_1)),
unique_words_repeated_fromfirst = c(0,
                            length(unique(allwords_2[allwords_2 %in% allwords_1])),
                            length(unique(allwords_3[allwords_3 %in% allwords_1])),
                            length(unique(allwords_4[allwords_4 %in% allwords_1]))
                            ),
words_repeated_fromsecond = c(0, 0, 
                            sum(allwords_3 %in% allwords_2),
                            sum(allwords_4 %in% allwords_2)),
unique_words_repeated_fromsecond = c(0, 0,
                            length(unique(allwords_3[allwords_3 %in% allwords_2])),
                            length(unique(allwords_4[allwords_4 %in% allwords_2]))
                            ),
words_repeated_fromthird = c(0, 0, 0,
                            sum(allwords_4 %in% allwords_3)),
unique_words_repeated_fromthird = c(0, 0, 0,
                            length(unique(allwords_4[allwords_4 %in% allwords_3]))
                            )   
, stringsAsFactors = FALSE)
return(df_insights)
}

# going to create a data frame with all the counts from the top candidates...
candidates &amp;lt;- c(&amp;quot;TRUMP&amp;quot;,&amp;quot;CARSON&amp;quot;,&amp;quot;RUBIO&amp;quot;,
                &amp;quot;KASICH&amp;quot;,&amp;quot;CRUZ&amp;quot;,&amp;quot;BUSH&amp;quot;,
                &amp;quot;FIORINA&amp;quot;,&amp;quot;PAUL&amp;quot;,&amp;quot;CHRISTIE&amp;quot;)
info &amp;lt;- NULL
info_all &amp;lt;- NULL
for(i in 1:9){
info &amp;lt;- UnlistAndExtractInfo(candidates[i])
info$CANDIDATE &amp;lt;- candidates[i]
info_all &amp;lt;- rbind(info_all, info)
}

# i&amp;#39;m going to add a few more columns...
info_all %&amp;lt;&amp;gt;% mutate(carry_over_p1 = unique_words_repeated_fromfirst/words_unique,
                     word_repeat = words_total/words_unique)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s make some nice graphs with this information…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# graph of most words spoken by debate
ggplot(order_axis(
  subset(info_all, debate != &amp;quot;Ohio&amp;quot; &amp;amp; CANDIDATE != &amp;quot;CHRISTIE&amp;quot;), # christie didn&amp;#39;t go to wisconsin
    CANDIDATE, carry_over_p1), 
       aes(x = CANDIDATE_o, 
           y = carry_over_p1)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;, 
           aes(fill = CANDIDATE_o)) + 
  facet_grid(debate ~.) + 
  theme_eem() +
  scale_fill_eem(20) + 
  labs(title = &amp;quot;Repetition of words by candidate&amp;quot;, 
       x = &amp;quot;Candidate&amp;quot;, 
       y = &amp;quot;% of unique words repeated from first debate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If we take the full amount of unique words from the first debate, it’s clear the candidates haven’t been saying very different things. For example in California (the second debate), 41% of the words Trump said were the same he said in Ohio. The Donald is arguably the most repetitive, increasing this to 49% and 48% in Colorado and Wisconsin.&lt;/p&gt;

&lt;p&gt;On the other hand, Fiorina always seems to have a surprise for viewers…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/repetitions.png&quot; alt=&quot;repetitions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although this could simply be due to the fact that she got very few words in the first debate (the outlier at the bottom is Fiorina) …&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot(subset(info_all,CANDIDATE != &amp;quot;CHRISTIE&amp;quot;), 
       aes(x = words_total, 
           y = words_unique)) + 
    geom_point(aes(colour = CANDIDATE), size = 3, shape = 2) +
    theme_eem()+ # uses &amp;quot;eflores/eem&amp;quot;
    scale_colour_eem(20) + # uses &amp;quot;eflores/eem&amp;quot;
    labs(title = &amp;quot;Words per Debate&amp;quot;,
         x = &amp;quot;Total Words&amp;quot;, 
         y = &amp;quot;Unique Words&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/words_v_unique.png&quot; alt=&quot;repetitions&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# average length of interventions
ggplot(info_all, 
       aes(x = factor(CANDIDATE), 
           y = average_intervention, 
                fill = eem_colors[1])) + # the eem colors are from &amp;quot;eflores/eem&amp;quot;
  geom_boxplot() +
  theme_eem()+
  labs(title = &amp;quot;Average words per intervention&amp;quot;,
       x = &amp;quot;Candidate&amp;quot;, 
       y = &amp;quot;Words&amp;quot;) + theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When it comes to the average length of “interventions” (I define one as the slot a candidate is speaking in without being interrupted), Fiorina and Trump like to keep it short and simple while Rubio takes his time…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/candidate_words.png&quot; alt=&quot;words_int&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# average times unique word is repeated...

ggplot(info_all, 
       aes(x = factor(CANDIDATE), 
           y = word_repeat, 
           fill = eem_colors[1])) +
  geom_boxplot() +
  theme_eem()+
  labs(title = &amp;quot;Average repetition of unique words&amp;quot;,
       x = &amp;quot;Candidate&amp;quot;, 
       y = &amp;quot;Repetitions&amp;quot;) + theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Another interesting tid-bit is the average repetitions of words. Again, Trump seems like an outlier, he repeated an average of 5 times every unique word in the California debate and has been repetitive since then.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/words_repeat.png&quot; alt=&quot;repetition_avg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A trend seems to be emerging: Trump repeats the same thing every debate. But there should be much more proof after a few more debates…&lt;/p&gt;

</description>
				<pubDate>Thu, 26 Nov 2015 00:00:00 +0100</pubDate>
				<link>http://localhost:4000/datascience/republican-debates/</link>
				<guid isPermaLink="true">http://localhost:4000/datascience/republican-debates/</guid>
			</item>
		
	</channel>
</rss>
